{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a6942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.0.44)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.9.11)\n",
      "Requirement already satisfied: pandas in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: dotenv in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (0.9.9)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from dotenv) (1.1.1)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 30.7/64.7 kB 660.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 64.7/64.7 kB 865.0 kB/s eta 0:00:00\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.3 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 133.1/163.3 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 163.3/163.3 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.0/71.0 kB 4.1 MB/s eta 0:00:00\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, idna, certifi, requests\n",
      "Successfully installed certifi-2025.10.5 idna-3.11 requests-2.32.5 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber sqlalchemy psycopg2-binary pandas dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ee287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão com PostgreSQL estabelecida com sucesso no schema 'FOPAG'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Carrega as variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# --- Carregue os componentes individuais ---\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "# (Opcional) Seu schema\n",
    "DB_SCHEMA = os.getenv('DB_SCHEMA') \n",
    "\n",
    "# --- Construa a DB_URL aqui no Python ---\n",
    "if not all([DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME]):\n",
    "    print(\"Erro: Faltando uma ou mais variáveis (DB_USER, DB_PASS, etc) no .env\")\n",
    "    exit()\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "# ------------------------------------------\n",
    "\n",
    "# Configurações globais da API\n",
    "# (A linha \"DB_URL = DB_URL\" era redundante e foi removida)\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token token={API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Cria a \"engine\" de conexão com o banco\n",
    "try:\n",
    "    # Opção para definir o schema padrão da conexão\n",
    "    engine = create_engine(\n",
    "        DB_URL,\n",
    "        connect_args={'options': f'-csearch_path={DB_SCHEMA}'}\n",
    "    )\n",
    "    print(f\"Conexão com PostgreSQL estabelecida com sucesso no schema '{DB_SCHEMA}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ao PostgreSQL: {e}\")\n",
    "    # Imprime a URL sem a senha para depuração\n",
    "    print(f\"String de conexão tentada: postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc714a",
   "metadata": {},
   "source": [
    "# Fase 1: Pipelines das dimensões (Dados da API)\n",
    "\n",
    "## Passo 1.A: Pipelie da ```dim_departamentos```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6519bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ETL...\n",
      "Conexão com PostgreSQL estabelecida e schema '\"FOPAG\"' garantido.\n",
      "\n",
      "--- Iniciando Pipeline: dim_colaboradores ---\n",
      "Iniciando extração (Passo 1/2): Buscando lista de IDs de colaboradores...\n",
      "Página 1 da lista carregada...\n",
      "Página 2 da lista carregada...\n",
      "Extração da lista concluída. Total de 138 colaboradores encontrados.\n",
      "Passo 1/2 concluído. 138 colaboradores encontrados.\n",
      "Iniciando extração (Passo 2/2): Buscando detalhes completos...\n",
      "  Buscando colaborador 1 de 138 (ID: 3382492)...\n",
      "  Buscando colaborador 2 de 138 (ID: 805340)...\n",
      "  Buscando colaborador 3 de 138 (ID: 588289)...\n",
      "  Buscando colaborador 4 de 138 (ID: 1440231)...\n",
      "  Buscando colaborador 5 de 138 (ID: 588290)...\n",
      "  Buscando colaborador 6 de 138 (ID: 2301298)...\n",
      "  Buscando colaborador 7 de 138 (ID: 2048031)...\n",
      "  Buscando colaborador 8 de 138 (ID: 2050250)...\n",
      "  Buscando colaborador 9 de 138 (ID: 2284524)...\n",
      "  Buscando colaborador 10 de 138 (ID: 805341)...\n",
      "  Buscando colaborador 11 de 138 (ID: 2274831)...\n",
      "  Buscando colaborador 12 de 138 (ID: 3580987)...\n",
      "  Buscando colaborador 13 de 138 (ID: 1643216)...\n",
      "  Buscando colaborador 14 de 138 (ID: 2566389)...\n",
      "  Buscando colaborador 15 de 138 (ID: 2866923)...\n",
      "  Buscando colaborador 16 de 138 (ID: 3506783)...\n",
      "  Buscando colaborador 17 de 138 (ID: 2273060)...\n",
      "  Buscando colaborador 18 de 138 (ID: 2301299)...\n",
      "  Buscando colaborador 19 de 138 (ID: 2274832)...\n",
      "  Buscando colaborador 20 de 138 (ID: 805342)...\n",
      "  Buscando colaborador 21 de 138 (ID: 1643238)...\n",
      "  Buscando colaborador 22 de 138 (ID: 3756597)...\n",
      "  Buscando colaborador 23 de 138 (ID: 805337)...\n",
      "  Buscando colaborador 24 de 138 (ID: 1971216)...\n",
      "  Buscando colaborador 25 de 138 (ID: 588293)...\n",
      "  Buscando colaborador 26 de 138 (ID: 1941348)...\n",
      "  Buscando colaborador 27 de 138 (ID: 1763453)...\n",
      "  Buscando colaborador 28 de 138 (ID: 588295)...\n",
      "  Buscando colaborador 29 de 138 (ID: 2301300)...\n",
      "  Buscando colaborador 30 de 138 (ID: 1637579)...\n",
      "  Buscando colaborador 31 de 138 (ID: 1562180)...\n",
      "  Buscando colaborador 32 de 138 (ID: 2301301)...\n",
      "  Buscando colaborador 33 de 138 (ID: 588296)...\n",
      "  Buscando colaborador 34 de 138 (ID: 1371885)...\n",
      "  Buscando colaborador 35 de 138 (ID: 2606702)...\n",
      "  Buscando colaborador 36 de 138 (ID: 805349)...\n",
      "  Buscando colaborador 37 de 138 (ID: 3612439)...\n",
      "  Buscando colaborador 38 de 138 (ID: 2642892)...\n",
      "  Buscando colaborador 39 de 138 (ID: 588298)...\n",
      "  Buscando colaborador 40 de 138 (ID: 2054652)...\n",
      "  Buscando colaborador 41 de 138 (ID: 3317882)...\n",
      "  Buscando colaborador 42 de 138 (ID: 3315272)...\n",
      "  Buscando colaborador 43 de 138 (ID: 588299)...\n",
      "  Buscando colaborador 44 de 138 (ID: 3506896)...\n",
      "  Buscando colaborador 45 de 138 (ID: 2150037)...\n",
      "  Buscando colaborador 46 de 138 (ID: 588301)...\n",
      "  Buscando colaborador 47 de 138 (ID: 588302)...\n",
      "  Buscando colaborador 48 de 138 (ID: 2590715)...\n",
      "  Buscando colaborador 49 de 138 (ID: 3196293)...\n",
      "  Buscando colaborador 50 de 138 (ID: 698273)...\n",
      "  Buscando colaborador 51 de 138 (ID: 1865774)...\n",
      "  Buscando colaborador 52 de 138 (ID: 2301303)...\n",
      "  Buscando colaborador 53 de 138 (ID: 3315538)...\n",
      "  Buscando colaborador 54 de 138 (ID: 588288)...\n",
      "  Buscando colaborador 55 de 138 (ID: 3734461)...\n",
      "  Buscando colaborador 56 de 138 (ID: 1763391)...\n",
      "  Buscando colaborador 57 de 138 (ID: 588303)...\n",
      "  Buscando colaborador 58 de 138 (ID: 2301304)...\n",
      "  Buscando colaborador 59 de 138 (ID: 3369826)...\n",
      "  Buscando colaborador 60 de 138 (ID: 588304)...\n",
      "  Buscando colaborador 61 de 138 (ID: 2301305)...\n",
      "  Buscando colaborador 62 de 138 (ID: 1763328)...\n",
      "  Buscando colaborador 63 de 138 (ID: 1643256)...\n",
      "  Buscando colaborador 64 de 138 (ID: 2301306)...\n",
      "  Buscando colaborador 65 de 138 (ID: 2392580)...\n",
      "  Buscando colaborador 66 de 138 (ID: 2301307)...\n",
      "  Buscando colaborador 67 de 138 (ID: 2759352)...\n",
      "  Buscando colaborador 68 de 138 (ID: 2301308)...\n",
      "  Buscando colaborador 69 de 138 (ID: 588305)...\n",
      "  Buscando colaborador 70 de 138 (ID: 1371868)...\n",
      "  Buscando colaborador 71 de 138 (ID: 1801211)...\n",
      "  Buscando colaborador 72 de 138 (ID: 3248097)...\n",
      "  Buscando colaborador 73 de 138 (ID: 2301309)...\n",
      "  Buscando colaborador 74 de 138 (ID: 1941283)...\n",
      "  Buscando colaborador 75 de 138 (ID: 2744573)...\n",
      "  Buscando colaborador 76 de 138 (ID: 2342194)...\n",
      "  Buscando colaborador 77 de 138 (ID: 588307)...\n",
      "  Buscando colaborador 78 de 138 (ID: 3192186)...\n",
      "  Buscando colaborador 79 de 138 (ID: 3775218)...\n",
      "  Buscando colaborador 80 de 138 (ID: 2339246)...\n",
      "  Buscando colaborador 81 de 138 (ID: 2117091)...\n",
      "  Buscando colaborador 82 de 138 (ID: 3314591)...\n",
      "  Buscando colaborador 83 de 138 (ID: 1549824)...\n",
      "  Buscando colaborador 84 de 138 (ID: 2115470)...\n",
      "  Buscando colaborador 85 de 138 (ID: 2301310)...\n",
      "  Buscando colaborador 86 de 138 (ID: 2301311)...\n",
      "  Buscando colaborador 87 de 138 (ID: 2714898)...\n",
      "  Buscando colaborador 88 de 138 (ID: 2012187)...\n",
      "  Buscando colaborador 89 de 138 (ID: 1998927)...\n",
      "  Buscando colaborador 90 de 138 (ID: 2953074)...\n",
      "  Buscando colaborador 91 de 138 (ID: 2661253)...\n",
      "  Buscando colaborador 92 de 138 (ID: 805344)...\n",
      "  Buscando colaborador 93 de 138 (ID: 1763410)...\n",
      "  Buscando colaborador 94 de 138 (ID: 2301326)...\n",
      "  Buscando colaborador 95 de 138 (ID: 2179880)...\n",
      "  Buscando colaborador 96 de 138 (ID: 2301312)...\n",
      "  Buscando colaborador 97 de 138 (ID: 2187015)...\n",
      "  Buscando colaborador 98 de 138 (ID: 2635725)...\n",
      "  Buscando colaborador 99 de 138 (ID: 588310)...\n",
      "  Buscando colaborador 100 de 138 (ID: 2631651)...\n",
      "  Buscando colaborador 101 de 138 (ID: 805345)...\n",
      "  Buscando colaborador 102 de 138 (ID: 2301313)...\n",
      "  Buscando colaborador 103 de 138 (ID: 2301314)...\n",
      "  Buscando colaborador 104 de 138 (ID: 2301315)...\n",
      "  Buscando colaborador 105 de 138 (ID: 588311)...\n",
      "  Buscando colaborador 106 de 138 (ID: 1549844)...\n",
      "  Buscando colaborador 107 de 138 (ID: 698344)...\n",
      "  Buscando colaborador 108 de 138 (ID: 588312)...\n",
      "  Buscando colaborador 109 de 138 (ID: 2301316)...\n",
      "  Buscando colaborador 110 de 138 (ID: 698223)...\n",
      "  Buscando colaborador 111 de 138 (ID: 1759190)...\n",
      "  Buscando colaborador 112 de 138 (ID: 1577940)...\n",
      "  Buscando colaborador 113 de 138 (ID: 588314)...\n",
      "  Buscando colaborador 114 de 138 (ID: 1763446)...\n",
      "  Buscando colaborador 115 de 138 (ID: 3382632)...\n",
      "  Buscando colaborador 116 de 138 (ID: 2339469)...\n",
      "  Buscando colaborador 117 de 138 (ID: 2187224)...\n",
      "  Buscando colaborador 118 de 138 (ID: 1559407)...\n",
      "  Buscando colaborador 119 de 138 (ID: 805346)...\n",
      "  Buscando colaborador 120 de 138 (ID: 588316)...\n",
      "  Buscando colaborador 121 de 138 (ID: 2301317)...\n",
      "  Buscando colaborador 122 de 138 (ID: 2050236)...\n",
      "  Buscando colaborador 123 de 138 (ID: 3164042)...\n",
      "  Buscando colaborador 124 de 138 (ID: 1371878)...\n",
      "  Buscando colaborador 125 de 138 (ID: 2729593)...\n",
      "  Buscando colaborador 126 de 138 (ID: 2087860)...\n",
      "  Buscando colaborador 127 de 138 (ID: 2115468)...\n",
      "  Buscando colaborador 128 de 138 (ID: 805331)...\n",
      "  Buscando colaborador 129 de 138 (ID: 3315338)...\n",
      "  Buscando colaborador 130 de 138 (ID: 2301318)...\n",
      "  Buscando colaborador 131 de 138 (ID: 2121504)...\n",
      "  Buscando colaborador 132 de 138 (ID: 2467732)...\n",
      "  Buscando colaborador 133 de 138 (ID: 588318)...\n",
      "  Buscando colaborador 134 de 138 (ID: 2041283)...\n",
      "  Buscando colaborador 135 de 138 (ID: 1801238)...\n",
      "  Buscando colaborador 136 de 138 (ID: 805347)...\n",
      "  Buscando colaborador 137 de 138 (ID: 588320)...\n",
      "  Buscando colaborador 138 de 138 (ID: 805348)...\n",
      "Passo 2/2 concluído. Detalhes de todos os colaboradores buscados.\n",
      "Info: Departamento encontrado na chave 'departament.name'.\n",
      "Info: Cargo encontrado na chave 'position.name'.\n",
      "Info: Nível Educacional encontrado na chave 'education'.\n",
      "Info: CPF encontrado na chave 'documents.idNumber'.\n",
      "Transformação de colaboradores concluída.\n",
      "Carga na staging de colaboradores concluída.\n",
      "Carga na dim_colaboradores concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: dim_calendario ---\n",
      "Carga na dim_calendario concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: fato_folha_consolidada ---\n",
      "Convertendo colunas de métricas...\n",
      "CSV 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv' lido e transformado.\n",
      "CSV carregado para staging_folha_consolidada.\n",
      "Carga na fato_folha_consolidada concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: fato_folha_detalhada ---\n",
      "CSV 'BASE_FOPAG_DETALHADA_RUBRICAS.csv' lido e transformado.\n",
      "CSV carregado para staging_folha_detalhada.\n",
      "Carga na fato_folha_detalhada concluída com sucesso!\n",
      "\n",
      "--- Pipeline ETL Concluído com Sucesso! ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, exc as sqlalchemy_exc\n",
    "import sys\n",
    "import numpy as np # Import numpy for NaN handling\n",
    "import json # <-- Import mantido, mas o debug foi removido\n",
    "\n",
    "# 1. CARREGAR VARIÁVEIS DE AMBIENTE\n",
    "# -----------------------------------\n",
    "print(\"Iniciando ETL...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Carrega o Token da API\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "\n",
    "# Carrega os componentes do Banco\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_SCHEMA = os.getenv('DB_SCHEMA')\n",
    "\n",
    "# Verifica se tudo foi carregado\n",
    "if not all([API_TOKEN, DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME, DB_SCHEMA]):\n",
    "    print(\"ERRO: Faltando uma ou mais variáveis no arquivo .env\")\n",
    "    print(f\"API_TOKEN Carregado: {'Sim' if API_TOKEN else 'NÃO'}\")\n",
    "    print(f\"DB_SCHEMA CarregADO: {DB_SCHEMA}\")\n",
    "    sys.exit() # Encerra o script se faltar configuração\n",
    "\n",
    "# 2. CONFIGURAÇÕES GLOBAIS\n",
    "# -----------------------------------\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "BASE_URL = \"https://app.solides.com/pt-BR/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token token={API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 3. CRIA A CONEXÃO E GARANTE O SCHEMA (COM ASPAS)\n",
    "# ----------------------------------------------------\n",
    "try:\n",
    "    engine = create_engine(DB_URL)\n",
    "    with engine.begin() as conn:\n",
    "        # Mantemos o unaccent para o caso de ser útil no futuro\n",
    "        conn.execute(text('CREATE EXTENSION IF NOT EXISTS unaccent;'))\n",
    "        conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \"{DB_SCHEMA}\"'))\n",
    "\n",
    "    # Recria a engine, definindo o search_path\n",
    "    engine = create_engine(\n",
    "        DB_URL,\n",
    "        connect_args={'options': f'-csearch_path=\"{DB_SCHEMA}\"'}\n",
    "    )\n",
    "\n",
    "    print(f\"Conexão com PostgreSQL estabelecida e schema '\\\"{DB_SCHEMA}\\\"' garantido.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ao PostgreSQL ou criar schema: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# --- FUNÇÃO HELPER PARA LIMPEZA ---\n",
    "def limpar_salario_api(salario_str):\n",
    "    \"\"\"Limpa a string de salário vinda da API (ex: \"R$ 8.200,00\") para float.\"\"\"\n",
    "    if salario_str is None or pd.isna(salario_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Remove 'R$', espaços, e usa '.' como separador de milhar\n",
    "        salario_limpo = str(salario_str).replace('R$', '').replace(' ', '').replace('.', '')\n",
    "        # Troca ',' por '.' para ser decimal\n",
    "        salario_limpo = salario_limpo.replace(',', '.')\n",
    "        return pd.to_numeric(salario_limpo, errors='coerce')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- FASE 1: PIPELINES DAS DIMENSÕES (API) ---\n",
    "\n",
    "def pipeline_dim_colaboradores():\n",
    "    \"\"\"\n",
    "    PUXA dados de Colaboradores da API (paginado) e carrega na dim_colaboradores.\n",
    "    Esta é a \"Dimensão Mestre\" de Colaboradores, com dados cadastrais.\n",
    "    \n",
    "    *** ATUALIZAÇÃO ***:\n",
    "    O pipeline agora é feito em 2 PASSOS:\n",
    "    1. Busca a LISTA de colaboradores (/colaboradores) para obter os IDs.\n",
    "    2. Itera sobre os IDs e busca os DADOS DETALHADOS de cada um (/colaboradores/{id})\n",
    "       para obter os dados pessoais (endereço, email, etc.).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_colaboradores ---\")\n",
    "\n",
    "    # 1. Extração (E) - PASSO 1: Obter IDs da lista\n",
    "    all_colaboradores_lista = []\n",
    "    page = 1\n",
    "    page_size = 100\n",
    "    ENDPOINT_LISTA = \"/colaboradores\" # [cite: openapi.json]\n",
    "\n",
    "    print(\"Iniciando extração (Passo 1/2): Buscando lista de IDs de colaboradores...\")\n",
    "    while True:\n",
    "        params = {'page': page, 'page_size': page_size, 'status': 'todos'} # [cite: openapi.json]\n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{ENDPOINT_LISTA}\", headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if not data:\n",
    "                    print(f\"Extração da lista concluída. Total de {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "                    break\n",
    "                all_colaboradores_lista.extend(data) # Contém dicts {'id': 123, 'name': '...'}\n",
    "                print(f\"Página {page} da lista carregada...\")\n",
    "                page += 1\n",
    "            else:\n",
    "                print(f\"Erro na API (Página {page}): {response.status_code} {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na extração de colaboradores (lista): {e}\")\n",
    "            return False\n",
    "\n",
    "    if not all_colaboradores_lista:\n",
    "        print(\"Nenhum colaborador encontrado.\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"Passo 1/2 concluído. {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "\n",
    "    # PASSO 2: Buscar detalhes de CADA colaborador\n",
    "    all_colaboradores_detalhado = []\n",
    "    total_colabs = len(all_colaboradores_lista)\n",
    "    \n",
    "    print(f\"Iniciando extração (Passo 2/2): Buscando detalhes completos...\")\n",
    "\n",
    "    for i, colab_info in enumerate(all_colaboradores_lista):\n",
    "        colab_id = colab_info.get('id')\n",
    "        if not colab_id:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Buscando colaborador {i+1} de {total_colabs} (ID: {colab_id})...\")\n",
    "        ENDPOINT_DETALHE = f\"/colaboradores/{colab_id}\"\n",
    "        \n",
    "        try:\n",
    "            # Adiciona um pequeno delay para não sobrecarregar a API\n",
    "            # time.sleep(0.1) # Descomente se encontrar erros de \"Too Many Requests\"\n",
    "            response_detalhe = requests.get(f\"{BASE_URL}{ENDPOINT_DETALHE}\", headers=HEADERS)\n",
    "            \n",
    "            if response_detalhe.status_code == 200:\n",
    "                data_detalhe = response_detalhe.json()\n",
    "                all_colaboradores_detalhado.append(data_detalhe)\n",
    "            else:\n",
    "                print(f\"    ERRO ao buscar detalhes do ID {colab_id}: {response_detalhe.status_code}. Usando dados básicos da lista.\")\n",
    "                # Se falhar, usamos a info básica da lista para não perder o funcionário\n",
    "                all_colaboradores_detalhado.append(colab_info) \n",
    "        except Exception as e:\n",
    "            print(f\"    EXCEÇÃO ao buscar detalhes do ID {colab_id}: {e}. Usando dados básicos da lista.\")\n",
    "            all_colaboradores_detalhado.append(colab_info) # Adiciona info básica\n",
    "\n",
    "    print(\"Passo 2/2 concluído. Detalhes de todos os colaboradores buscados.\")\n",
    "\n",
    "    # 2. Transformação (T)\n",
    "    # Agora usamos a lista 'all_colaboradores_detalhado' que contém os dados completos\n",
    "    df = pd.json_normalize(all_colaboradores_detalhado)\n",
    "\n",
    "    # --- Tentativa robusta de buscar o NOME DO DEPARTAMENTO ---\n",
    "    # O JSON confirmou que a chave é 'departament.name' (com 'a')\n",
    "    df['dept_name_temp'] = None\n",
    "    if 'departament.name' in df.columns: # Tenta com 'a'\n",
    "        print(\"Info: Departamento encontrado na chave 'departament.name'.\")\n",
    "        df['dept_name_temp'] = df['departament.name']\n",
    "    elif 'department.name' in df.columns: \n",
    "        print(\"Info: Departamento encontrado na chave 'department.name'.\")\n",
    "        df['dept_name_temp'] = df['department.name']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Departamento ('departament.name', 'department.name') foi encontrada.\")\n",
    "\n",
    "    # --- Tentativa robusta de buscar o NOME DO CARGO ---\n",
    "    # O JSON confirmou que a chave é 'position.name'\n",
    "    df['cargo_name_temp'] = None\n",
    "    if 'position.name' in df.columns:\n",
    "        print(\"Info: Cargo encontrado na chave 'position.name'.\")\n",
    "        df['cargo_name_temp'] = df['position.name']\n",
    "    elif 'cargo.name' in df.columns: # Tenta 'cargo'\n",
    "        print(\"Info: Cargo encontrado na chave 'cargo.name'.\")\n",
    "        df['cargo_name_temp'] = df['cargo.name']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Cargo ('position.name', 'cargo.name') foi encontrada.\")\n",
    "\n",
    "    # --- CORREÇÃO: Tentativa robusta de buscar o NÍVEL EDUCACIONAL ---\n",
    "    # O JSON confirmou que a chave é 'education'\n",
    "    df['education_level_temp'] = None\n",
    "    if 'education' in df.columns: # <-- A CHAVE CORRETA DO JSON\n",
    "        print(\"Info: Nível Educacional encontrado na chave 'education'.\")\n",
    "        df['education_level_temp'] = df['education']\n",
    "    elif 'educationLevel' in df.columns: \n",
    "        print(\"Info: Nível Educacional encontrado na chave 'educationLevel'.\")\n",
    "        df['education_level_temp'] = df['educationLevel']\n",
    "    elif 'scholarship' in df.columns: # Tentativa comum em PT-BR\n",
    "        print(\"Info: Nível Educacional encontrado na chave 'scholarship'.\")\n",
    "        df['education_level_temp'] = df['scholarship']\n",
    "    elif 'schooling' in df.columns:\n",
    "        print(\"Info: Nível Educacional encontrado na chave 'schooling'.\")\n",
    "        df['education_level_temp'] = df['schooling']\n",
    "    elif 'escolaridade' in df.columns: # Tentativa direta em PT\n",
    "        print(\"Info: Nível Educacional encontrado na chave 'escolaridade'.\")\n",
    "        df['education_level_temp'] = df['escolaridade']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Nível Educacional ('education', 'educationLevel', 'scholarship', 'schooling', 'escolaridade') foi encontrada.\")\n",
    "    # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "    # --- CORREÇÃO: Tentativa robusta de buscar o CPF ---\n",
    "    # O JSON confirmou que a chave é 'documents.idNumber'\n",
    "    df['cpf_temp'] = None \n",
    "    \n",
    "    if 'documents.idNumber' in df.columns: # <-- A CHAVE CORRETA\n",
    "        print(\"Info: CPF encontrado na chave 'documents.idNumber'.\")\n",
    "        df['cpf_temp'] = df['documents.idNumber']\n",
    "    elif 'documents.cpf' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'documents.cpf'.\")\n",
    "        df['cpf_temp'] = df['documents.cpf']\n",
    "    elif 'idNumber' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'idNumber' (raiz).\")\n",
    "        df['cpf_temp'] = df['idNumber']\n",
    "    elif 'cpf' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'cpf' (raiz).\")\n",
    "        df['cpf_temp'] = df['cpf']\n",
    "    elif 'document' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'document' (raiz).\")\n",
    "        df['cpf_temp'] = df['document']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de CPF ('documents.idNumber', 'idNumber', 'cpf', 'document') foi encontrada.\")\n",
    "\n",
    "    # Limpa a coluna temporária (remove pontos, traços, etc.)\n",
    "    if 'cpf_temp' in df.columns:\n",
    "         df['cpf_temp'] = df['cpf_temp'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "         df['cpf_temp'] = df['cpf_temp'].replace(r'^\\s*$', np.nan, regex=True).replace('None', np.nan).replace('nan', np.nan)\n",
    "    else:\n",
    "         df['cpf_temp'] = None\n",
    "    # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "    # --- ADIÇÃO: Limpeza do Salário da API ---\n",
    "    if 'salary' in df.columns:\n",
    "        df['salario_api_temp'] = df['salary'].apply(limpar_salario_api)\n",
    "    else:\n",
    "        df['salario_api_temp'] = np.nan\n",
    "    # --- FIM DA ADIÇÃO ---\n",
    "\n",
    "\n",
    "    # --- ATUALIZAÇÃO: Renomeia todas as colunas (antigas e novas) ---\n",
    "    df = df.rename(columns={\n",
    "        'id': 'colaborador_id_solides',\n",
    "        'name': 'nome_completo',\n",
    "        'cpf_temp': 'cpf', \n",
    "        'birthDate': 'data_nascimento',\n",
    "        'gender': 'genero',\n",
    "        'dateAdmission': 'data_admissao',\n",
    "        'dateDismissal': 'data_demissao',\n",
    "        'active': 'ativo',\n",
    "        'dept_name_temp': 'departamento_nome_api', \n",
    "        'cargo_name_temp': 'cargo_nome_api',      \n",
    "        'email': 'email',\n",
    "        'contact.phone': 'telefone_pessoal', # <-- Chave correta do JSON\n",
    "        'contact.cellPhone': 'celular', # <-- Chave correta do JSON\n",
    "        'nationality': 'nacionalidade',\n",
    "        'education_level_temp': 'nivel_educacional', # <-- Chave correta\n",
    "        'motherName': 'nome_mae',\n",
    "        'fatherName': 'nome_pai',\n",
    "        'address.streetName': 'logradouro', # <-- Chave correta do JSON\n",
    "        'address.number': 'numero_endereco',\n",
    "        'address.additionalInformation': 'complemento_endereco', # <-- Chave correta do JSON\n",
    "        'address.neighborhood': 'bairro',\n",
    "        'address.city.name': 'cidade', # <-- Chave correta do JSON\n",
    "        'address.state.initials': 'estado', # <-- Chave correta do JSON\n",
    "        'address.zipCode': 'cep',\n",
    "        \n",
    "        # --- NOVOS CAMPOS DE PEOPLE ANALYTICS ---\n",
    "        'registration': 'matricula',\n",
    "        'maritalStatus': 'estado_civil',\n",
    "        'salario_api_temp': 'salario_api',\n",
    "        'workShift': 'turno_trabalho',\n",
    "        'typeContract': 'tipo_contrato',\n",
    "        'course': 'curso_formacao',\n",
    "        'hierarchicalLevel': 'nivel_hierarquico',\n",
    "        'senior.name': 'nome_lider_imediato',\n",
    "        'ethnicity': 'etnia',\n",
    "        'unity.name': 'unidade_nome'\n",
    "    })\n",
    "\n",
    "    date_cols = ['data_nascimento', 'data_admissao', 'data_demissao']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            # Tenta formatar 'dd/mm/YYYY'. Se falhar, tenta 'YYYY-mm-dd' (visto em 'dispatchDate' do JSON)\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='raise')\n",
    "            except ValueError:\n",
    "                 df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='coerce')\n",
    "        else:\n",
    "             df[col] = pd.NaT\n",
    "\n",
    "    # --- ATUALIZAÇÃO: Lista de colunas final (antigas e novas) ---\n",
    "    colunas_staging = [\n",
    "        'colaborador_id_solides', 'cpf', 'nome_completo', 'data_nascimento', 'genero',\n",
    "        'data_admissao', 'data_demissao', 'ativo',\n",
    "        'departamento_nome_api', 'cargo_nome_api',\n",
    "        'email', 'telefone_pessoal', 'celular', 'nacionalidade', 'nivel_educacional',\n",
    "        'nome_mae', 'nome_pai',\n",
    "        'logradouro', 'numero_endereco', 'complemento_endereco', 'bairro', 'cidade', 'estado', 'cep',\n",
    "        \n",
    "        # --- NOVOS CAMPOS DE PEOPLE ANALYTICS ---\n",
    "        'matricula', 'estado_civil', 'salario_api', 'turno_trabalho', 'tipo_contrato',\n",
    "        'curso_formacao', 'nivel_hierarquico', 'nome_lider_imediato', 'etnia', 'unidade_nome'\n",
    "    ]\n",
    "    \n",
    "    for col in colunas_staging:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None \n",
    "\n",
    "    df_staging = df[colunas_staging].copy()\n",
    "    print(\"Transformação de colaboradores concluída.\")\n",
    "\n",
    "    # 3. Carga (L)\n",
    "    NOME_TABELA_FINAL = \"dim_colaboradores\"\n",
    "    NOME_TABELA_STAGING = \"staging_colaboradores\"\n",
    "\n",
    "    try:\n",
    "        df_staging.to_sql(NOME_TABELA_STAGING, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "        print(\"Carga na staging de colaboradores concluída.\")\n",
    "\n",
    "        # --- ATUALIZAÇÃO: SQL com todas as colunas novas ---\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk SERIAL PRIMARY KEY,\n",
    "            colaborador_id_solides INTEGER UNIQUE NOT NULL,\n",
    "            cpf VARCHAR(11), \n",
    "            nome_completo VARCHAR(255),\n",
    "            data_nascimento DATE,\n",
    "            genero VARCHAR(50),\n",
    "            data_admissao DATE,\n",
    "            data_demissao DATE,\n",
    "            ativo BOOLEAN,\n",
    "            departamento_nome_api VARCHAR(255),\n",
    "            cargo_nome_api VARCHAR(255),\n",
    "            email VARCHAR(255),\n",
    "            telefone_pessoal VARCHAR(50),\n",
    "            celular VARCHAR(50),\n",
    "            nacionalidade VARCHAR(100),\n",
    "            nivel_educacional VARCHAR(100),\n",
    "            nome_mae VARCHAR(255),\n",
    "            nome_pai VARCHAR(255),\n",
    "            logradouro VARCHAR(255),\n",
    "            numero_endereco VARCHAR(50),\n",
    "            complemento_endereco VARCHAR(100),\n",
    "            bairro VARCHAR(100),\n",
    "            cidade VARCHAR(100),\n",
    "            estado VARCHAR(50),\n",
    "            cep VARCHAR(20),\n",
    "            \n",
    "            -- NOVAS COLUNAS\n",
    "            matricula VARCHAR(50),\n",
    "            estado_civil VARCHAR(50),\n",
    "            salario_api NUMERIC(12, 2),\n",
    "            turno_trabalho VARCHAR(100),\n",
    "            tipo_contrato VARCHAR(100),\n",
    "            curso_formacao VARCHAR(255),\n",
    "            nivel_hierarquico VARCHAR(100),\n",
    "            nome_lider_imediato VARCHAR(255),\n",
    "            etnia VARCHAR(50),\n",
    "            unidade_nome VARCHAR(255),\n",
    "            \n",
    "            data_ultima_atualizacao TIMESTAMP DEFAULT current_timestamp\n",
    "        );\n",
    "\n",
    "        -- *** CORREÇÃO DO ERRO 'coluna \"matricula\" ... não existe' ***\n",
    "        -- Este bloco ALTER TABLE garante que o schema da tabela existente\n",
    "        -- seja atualizado ANTES de tentar inserir os dados.\n",
    "        ALTER TABLE \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "            ADD COLUMN IF NOT EXISTS matricula VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS estado_civil VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS salario_api NUMERIC(12, 2),\n",
    "            ADD COLUMN IF NOT EXISTS turno_trabalho VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS tipo_contrato VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS curso_formacao VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS nivel_hierarquico VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS nome_lider_imediato VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS etnia VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS unidade_nome VARCHAR(255);\n",
    "        -- *** FIM DA CORREÇÃO ***\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk, colaborador_id_solides, cpf, nome_completo, departamento_nome_api, cargo_nome_api\n",
    "        )\n",
    "        VALUES (0, -1, 'N/A', 'Desconhecido', 'Desconhecido', 'Desconhecido')\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_id_solides, cpf, nome_completo, data_nascimento, genero,\n",
    "            data_admissao, data_demissao, ativo,\n",
    "            departamento_nome_api, cargo_nome_api,\n",
    "            email, telefone_pessoal, celular, nacionalidade, nivel_educacional,\n",
    "            nome_mae, nome_pai,\n",
    "            logradouro, numero_endereco, complemento_endereco, bairro, cidade, estado, cep,\n",
    "            \n",
    "            -- NOVAS COLUNAS\n",
    "            matricula, estado_civil, salario_api, turno_trabalho, tipo_contrato,\n",
    "            curso_formacao, nivel_hierarquico, nome_lider_imediato, etnia, unidade_nome,\n",
    "            \n",
    "            data_ultima_atualizacao\n",
    "        )\n",
    "        SELECT\n",
    "            stg.colaborador_id_solides, \n",
    "            stg.cpf,\n",
    "            stg.nome_completo, stg.data_nascimento, stg.genero,\n",
    "            stg.data_admissao, stg.data_demissao, stg.ativo,\n",
    "            stg.departamento_nome_api,\n",
    "            stg.cargo_nome_api,\n",
    "            stg.email, stg.telefone_pessoal, stg.celular, stg.nacionalidade, stg.nivel_educacional,\n",
    "            stg.nome_mae, stg.nome_pai,\n",
    "            stg.logradouro, stg.numero_endereco, stg.complemento_endereco, stg.bairro, stg.cidade, stg.estado, stg.cep,\n",
    "\n",
    "            -- NOVAS COLUNAS\n",
    "            stg.matricula, stg.estado_civil, stg.salario_api, stg.turno_trabalho, stg.tipo_contrato,\n",
    "            stg.curso_formacao, stg.nivel_hierarquico, stg.nome_lider_imediato, stg.etnia, stg.unidade_nome,\n",
    "\n",
    "            current_timestamp\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "\n",
    "        ON CONFLICT (colaborador_id_solides) DO UPDATE SET\n",
    "            cpf = EXCLUDED.cpf, \n",
    "            nome_completo = EXCLUDED.nome_completo,\n",
    "            data_nascimento = EXCLUDED.data_nascimento,\n",
    "            genero = EXCLUDED.genero,\n",
    "            data_admissao = EXCLUDED.data_admissao,\n",
    "            data_demissao = EXCLUDED.data_demissao,\n",
    "            ativo = EXCLUDED.ativo,\n",
    "            departamento_nome_api = EXCLUDED.departamento_nome_api,\n",
    "            cargo_nome_api = EXCLUDED.cargo_nome_api,\n",
    "            email = EXCLUDED.email,\n",
    "            telefone_pessoal = EXCLUDED.telefone_pessoal,\n",
    "            celular = EXCLUDED.celular,\n",
    "            nacionalidade = EXCLUDED.nacionalidade,\n",
    "            nivel_educacional = EXCLUDED.nivel_educacional,\n",
    "            nome_mae = EXCLUDED.nome_mae,\n",
    "            nome_pai = EXCLUDED.nome_pai,\n",
    "            logradouro = EXCLUDED.logradouro,\n",
    "            numero_endereco = EXCLUDED.numero_endereco,\n",
    "            complemento_endereco = EXCLUDED.complemento_endereco,\n",
    "            bairro = EXCLUDED.bairro,\n",
    "            cidade = EXCLUDED.cidade,\n",
    "            estado = EXCLUDED.estado,\n",
    "            cep = EXCLUDED.cep,\n",
    "            \n",
    "            -- NOVAS COLUNAS\n",
    "            matricula = EXCLUDED.matricula,\n",
    "            estado_civil = EXCLUDED.estado_civil,\n",
    "            salario_api = EXCLUDED.salario_api,\n",
    "            turno_trabalho = EXCLUDED.turno_trabalho,\n",
    "            tipo_contrato = EXCLUDED.tipo_contrato,\n",
    "            curso_formacao = EXCLUDED.curso_formacao,\n",
    "            nivel_hierarquico = EXCLUDED.nivel_hierarquico,\n",
    "            nome_lider_imediato = EXCLUDED.nome_lider_imediato,\n",
    "            etnia = EXCLUDED.etnia,\n",
    "            unidade_nome = EXCLUDED.unidade_nome,\n",
    "            \n",
    "            data_ultima_atualizacao = current_timestamp;\n",
    "        \"\"\"\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_FINAL}: {e}\")\n",
    "        print(f\"Detalhe do erro: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- NOVA DIMENSÃO: CALENDÁRIO ---\n",
    "def pipeline_dim_calendario():\n",
    "    \"\"\"Gera ou atualiza a dimensão de calendário (dim_calendario).\"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_calendario ---\")\n",
    "    \n",
    "    NOME_TABELA_FINAL = \"dim_calendario\"\n",
    "    \n",
    "    # SQL para criar a tabela e popular com datas\n",
    "    # Gera datas de 2023 (início dos dados FOPAG) até 2030\n",
    "    sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "        data DATE PRIMARY KEY,\n",
    "        ano INTEGER,\n",
    "        mes INTEGER,\n",
    "        dia INTEGER,\n",
    "        trimestre INTEGER,\n",
    "        semestre INTEGER,\n",
    "        dia_da_semana INTEGER, -- 0=Dom, 1=Seg, ... 6=Sab\n",
    "        nome_dia_da_semana VARCHAR(20),\n",
    "        nome_mes VARCHAR(20),\n",
    "        nome_mes_abrev CHAR(3),\n",
    "        ano_mes VARCHAR(7), -- YYYY-MM\n",
    "        dia_do_ano INTEGER,\n",
    "        semana_do_ano INTEGER\n",
    "    );\n",
    "\n",
    "    -- Define o início e o fim do calendário\n",
    "    DO $$\n",
    "    DECLARE\n",
    "        data_inicio DATE := '2023-01-01'; -- Baseado nos dados da FOPAG\n",
    "        data_fim DATE := '2030-12-31';\n",
    "    BEGIN\n",
    "        -- Tenta definir o locale para Português para esta transação\n",
    "        -- Se 'pt_BR.UTF-8' não estiver disponível no servidor, 'pt_BR' é uma alternativa.\n",
    "        BEGIN\n",
    "            SET LOCAL lc_time = 'pt_BR.UTF-8';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            BEGIN\n",
    "                SET LOCAL lc_time = 'pt_BR';\n",
    "            EXCEPTION WHEN OTHERS THEN\n",
    "                -- Se ambos falharem, continua com o padrão do DB (provavelmente inglês)\n",
    "                RAISE NOTICE 'Não foi possível definir o locale pt_BR. Nomes de mês/dia podem ficar em inglês.';\n",
    "            END;\n",
    "        END;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            data,\n",
    "            ano,\n",
    "            mes,\n",
    "            dia,\n",
    "            trimestre,\n",
    "            semestre,\n",
    "            dia_da_semana,\n",
    "            nome_dia_da_semana,\n",
    "            nome_mes,\n",
    "            nome_mes_abrev,\n",
    "            ano_mes,\n",
    "            dia_do_ano,\n",
    "            semana_do_ano\n",
    "        )\n",
    "        SELECT\n",
    "            d AS data,\n",
    "            EXTRACT(YEAR FROM d) AS ano,\n",
    "            EXTRACT(MONTH FROM d) AS mes,\n",
    "            EXTRACT(DAY FROM d) AS dia,\n",
    "            EXTRACT(QUARTER FROM d) AS trimestre,\n",
    "            CASE WHEN EXTRACT(MONTH FROM d) <= 6 THEN 1 ELSE 2 END AS semestre,\n",
    "            EXTRACT(DOW FROM d) AS dia_da_semana, -- 0=Domingo, 6=Sábado\n",
    "            to_char(d, 'TMDay') AS nome_dia_da_semana,\n",
    "            to_char(d, 'TMMonth') AS nome_mes,\n",
    "            to_char(d, 'TMMon') AS nome_mes_abrev,\n",
    "            to_char(d, 'YYYY-MM') AS ano_mes,\n",
    "            EXTRACT(DOY FROM d) AS dia_do_ano,\n",
    "            EXTRACT(WEEK FROM d) AS semana_do_ano\n",
    "        FROM generate_series(data_inicio, data_fim, '1 day'::interval) d\n",
    "        ON CONFLICT (data) DO NOTHING; -- Torna a carga idempotente (não insere duplicados)\n",
    "    END $$;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_FINAL}: {e}\")\n",
    "        print(f\"Detalhe do erro: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- FASE 2: PIPELINES DAS FATOS (CSV) ---\n",
    "\n",
    "def converter_para_numero(coluna):\n",
    "    \"\"\"Limpa coluna de texto (ex: '10,000.50' ou '9677.42') para um número.\"\"\"\n",
    "    if coluna is None or coluna.empty:\n",
    "        return pd.Series(index=coluna.index, dtype=float) if isinstance(coluna, pd.Series) else None\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(coluna) and not pd.api.types.is_object_dtype(coluna):\n",
    "        return coluna\n",
    "    \n",
    "    coluna_str = coluna.astype(str)\n",
    "\n",
    "    # Assume formato Padrão/EUA (ex: 10,000.50 ou 9677.42)\n",
    "    # 1. Remove vírgulas (que seriam separador de milhar)\n",
    "    try:\n",
    "        coluna_str = coluna_str.str.replace(r',', '', regex=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    resultado = pd.to_numeric(coluna_str, errors='coerce')\n",
    "    return resultado\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_consolidada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_consolidada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_consolidada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_consolidada'\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            dtype_map = {}\n",
    "            \n",
    "            try:\n",
    "                header_df = pd.read_csv(CSV_FILE, sep=';', nrows=0)\n",
    "                if 'cargo' in header_df.columns:\n",
    "                    dtype_map['cargo'] = str\n",
    "                if 'departamento' in header_df.columns: # 'departamento' aqui é o Centro de Custo\n",
    "                   dtype_map['departamento'] = str\n",
    "                if 'nome_funcionario' in header_df.columns:\n",
    "                   dtype_map['nome_funcionario'] = str\n",
    "            except Exception:\n",
    "                pass \n",
    "\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=dtype_map)\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}. Tentando leitura simples.\")\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';')\n",
    "\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        if 'cpf' in df_csv.columns:\n",
    "            df_csv['cpf'] = df_csv['cpf'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        else:\n",
    "            print(f\"Aviso: Coluna 'cpf' não encontrada no CSV {CSV_FILE}.\")\n",
    "            df_csv['cpf'] = None\n",
    "\n",
    "        colunas_metricas = [\n",
    "            'salario_contratual', 'total_proventos', 'total_descontos',\n",
    "            'valor_liquido', 'base_inss', 'base_fgts', 'valor_fgts', 'base_irrf'\n",
    "        ]\n",
    "        print(\"Convertendo colunas de métricas...\")\n",
    "        for col in colunas_metricas:\n",
    "            if col in df_csv.columns:\n",
    "                df_csv[col] = converter_para_numero(df_csv[col])\n",
    "            else:\n",
    "                df_csv[col] = np.nan\n",
    "\n",
    "        if 'departamento' not in df_csv.columns:\n",
    "             print(f\"Aviso: Coluna 'departamento' (Centro de Custo) não encontrada no CSV {CSV_FILE}.\")\n",
    "             df_csv['departamento'] = None\n",
    "        if 'cargo' not in df_csv.columns:\n",
    "             print(f\"Aviso: Coluna 'cargo' (do CSV) não encontrada.\")\n",
    "             df_csv['cargo'] = None\n",
    "        if 'nome_funcionario' not in df_csv.columns:\n",
    "             print(f\"Aviso: Coluna 'nome_funcionario' (do CSV) não encontrada.\")\n",
    "             df_csv['nome_funcionario'] = None\n",
    "\n",
    "\n",
    "        print(f\"CSV '{CSV_FILE}' lido e transformado.\")\n",
    "\n",
    "\n",
    "        # 3. Carga (L)\n",
    "        df_csv.to_sql(NOME_TABELA_STAGING, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "\n",
    "        # *** Arquitetura Mantida: Sem JOIN no ETL ***\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_folha_id SERIAL PRIMARY KEY,\n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), -- Nome do CSV\n",
    "            centro_de_custo VARCHAR(255), -- Coluna para o 'departamento' do CSV\n",
    "            cargo_nome_csv VARCHAR(255),  -- Coluna para o 'cargo' do CSV\n",
    "            \n",
    "            -- *** Adicionada a coluna CPF do CSV ***\n",
    "            cpf_csv VARCHAR(11),\n",
    "\n",
    "            salario_contratual NUMERIC(12, 2),\n",
    "            total_proventos NUMERIC(12, 2),\n",
    "            total_descontos NUMERIC(12, 2),\n",
    "            valor_liquido NUMERIC(12, 2),\n",
    "            base_inss NUMERIC(12, 2),\n",
    "            base_fgts NUMERIC(12, 2),\n",
    "            valor_fgts NUMERIC(12, 2),\n",
    "            base_irrf NUMERIC(12, 2)\n",
    "        );\n",
    "\n",
    "        -- Carga Incremental: Deleta apenas as competências do arquivo\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN (\n",
    "            SELECT DISTINCT\n",
    "                CASE\n",
    "                    WHEN stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "                    THEN to_date(stg.competencia, 'YYYY-MM-DD')\n",
    "                    ELSE NULL\n",
    "                END\n",
    "            FROM \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "            WHERE stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "        );\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cargo_nome_csv, cpf_csv,\n",
    "            salario_contratual, total_proventos, total_descontos, valor_liquido,\n",
    "            base_inss, base_fgts, valor_fgts, base_irrf\n",
    "        )\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "                THEN to_date(stg.competencia, 'YYYY-MM-DD')\n",
    "                ELSE NULL\n",
    "            END AS competencia,\n",
    "\n",
    "            stg.nome_funcionario AS nome_funcionario_csv, -- Salva o nome do CSV\n",
    "            stg.departamento AS centro_de_custo, -- Salva o 'departamento' do CSV\n",
    "            stg.cargo AS cargo_nome_csv,          -- Salva o 'cargo' do CSV\n",
    "            stg.cpf AS cpf_csv,                   -- Salva o CPF do CSV\n",
    "\n",
    "            stg.salario_contratual, stg.total_proventos, stg.total_descontos, stg.valor_liquido,\n",
    "            stg.base_inss, stg.base_fgts, stg.valor_fgts, stg.base_irrf\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        ;\n",
    "        \"\"\"\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: # Captura erros do SQLAlchemy\n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: # Captura erros de leitura do CSV\n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: # Captura outros erros\n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_detalhada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_detalhada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_DETALHADA_RUBRICAS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_detalhada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_detalhada'\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            dtype_map = {}\n",
    "            try:\n",
    "                header_df = pd.read_csv(CSV_FILE, sep=';', nrows=0)\n",
    "                if 'departamento' in header_df.columns: \n",
    "                    dtype_map['departamento'] = str\n",
    "                if 'nome_funcionario' in header_df.columns:\n",
    "                   dtype_map['nome_funcionario'] = str\n",
    "            except Exception:\n",
    "                pass\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=dtype_map)\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}. Tentando leitura simples.\")\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';') # Fallback\n",
    "\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        if 'cpf' in df_csv.columns:\n",
    "            df_csv['cpf'] = df_csv['cpf'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        else:\n",
    "             print(f\"Aviso: Coluna 'cpf' não encontrada no CSV {CSV_FILE}.\")\n",
    "             df_csv['cpf'] = None\n",
    "\n",
    "        if 'valor_rubrica' in df_csv.columns:\n",
    "            df_csv['valor_rubrica'] = converter_para_numero(df_csv['valor_rubrica'])\n",
    "        else:\n",
    "            df_csv['valor_rubrica'] = np.nan \n",
    "\n",
    "        if 'departamento' not in df_csv.columns:\n",
    "             print(f\"Aviso: Coluna 'departamento' (Centro de Custo) não encontrada no CSV {CSV_FILE}. Será preenchida com None.\")\n",
    "             df_csv['departamento'] = None\n",
    "        if 'nome_funcionario' not in df_csv.columns:\n",
    "             print(f\"Aviso: Coluna 'nome_funcionario' (do CSV) não encontrada.\")\n",
    "             df_csv['nome_funcionario'] = None\n",
    "\n",
    "\n",
    "        print(f\"CSV '{CSV_FILE}' lido e transformado.\")\n",
    "\n",
    "        # 3. Carga (L)\n",
    "        df_csv.to_sql(NOME_TABELA_STAGING, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_rubrica_id SERIAL PRIMARY KEY,\n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), -- Nome do CSV\n",
    "            centro_de_custo VARCHAR(255), -- Coluna para o 'departamento' do CSV\n",
    "            \n",
    "            -- *** Adicionada a coluna CPF do CSV ***\n",
    "            cpf_csv VARCHAR(11),\n",
    "\n",
    "            codigo_rubrica VARCHAR(100),\n",
    "            nome_rubrica VARCHAR(255),\n",
    "            tipo_rubrica VARCHAR(100),\n",
    "            valor_rubrica NUMERIC(12, 2)\n",
    "        );\n",
    "\n",
    "        -- Carga Incremental: Deleta apenas as competências do arquivo\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN (\n",
    "            SELECT DISTINCT\n",
    "                CASE\n",
    "                    WHEN stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "                    THEN to_date(stg.competencia, 'YYYY-MM-DD')\n",
    "                    ELSE NULL\n",
    "                END\n",
    "            FROM \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "            WHERE stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "        );\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cpf_csv,\n",
    "            codigo_rubrica, nome_rubrica, tipo_rubrica, valor_rubrica\n",
    "        )\n",
    "        SELECT\n",
    "             CASE\n",
    "                 WHEN stg.competencia ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "                 THEN to_date(stg.competencia, 'YYYY-MM-DD')\n",
    "                 ELSE NULL\n",
    "             END AS competencia,\n",
    "            \n",
    "            stg.nome_funcionario AS nome_funcionario_csv, -- Salva o nome do CSV\n",
    "            stg.departamento AS centro_de_custo, -- Salva o 'departamento' do CSV\n",
    "            stg.cpf AS cpf_csv,                   -- Salva o CPF do CSV\n",
    "\n",
    "            stg.codigo_rubrica, stg.nome_rubrica, stg.tipo_rubrica, stg.valor_rubrica\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        ;\n",
    "        \"\"\"\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: # Captura erros do SQLAlchemy\n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: # Captura erros de leitura do CSV\n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: # Captura outros erros\n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- PONTO DE EXECUÇÃO PRINCIPAL ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ordem de execution é crucial\n",
    "\n",
    "    # 1. Dimensões independentes\n",
    "    sucesso_colab = pipeline_dim_colaboradores()\n",
    "    sucesso_calendario = pipeline_dim_calendario() # <-- NOVA DIMENSÃO\n",
    "\n",
    "    # 2. Fatos (dependem apenas de Colaboradores)\n",
    "    # As fatos também dependem do calendário, mas a FK é feita no BI.\n",
    "    if sucesso_colab and sucesso_calendario:\n",
    "        sucesso_fato_cons = pipeline_fato_folha_consolidada()\n",
    "        sucesso_fato_det = pipeline_fato_folha_detalhada()\n",
    "        if not sucesso_fato_cons or not sucesso_fato_det:\n",
    "             print(\"\\n!!! Atenção: Pelo menos um pipeline de FATO falhou. Verifique os logs acima. !!!\")\n",
    "        else:\n",
    "             print(\"\\n--- Pipeline ETL Concluído com Sucesso! ---\")\n",
    "\n",
    "    else:\n",
    "        if not sucesso_colab:\n",
    "             print(\"\\nFalha ao carregar dim_colaboradores. Abortando pipelines de Fatos.\")\n",
    "        if not sucesso_calendario:\n",
    "             print(\"\\nFalha ao carregar dim_calendario. Abortando pipelines de Fatos.\")\n",
    "        sys.exit() # Encerra se as dimensões falharem\n",
    "\n",
    "\n",
    "    # Fecha a conexão com o banco\n",
    "    engine.dispose()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
