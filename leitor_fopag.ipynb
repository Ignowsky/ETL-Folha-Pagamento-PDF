{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ignowsky/Payroll-PDF-Parser/blob/main/leitor_fopag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde78469",
      "metadata": {
        "id": "fde78469"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber sqlalchemy psycopg2-binary pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd3b981",
      "metadata": {
        "id": "9fd3b981"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ec1133",
      "metadata": {
        "id": "17ec1133",
        "outputId": "6971378e-039c-4ff5-b833-f423090e6eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encontrados 37 PDFs para processar...\n",
            "\n",
            "---> Processando arquivo: 01.2023 ARQDIGITAL - Folha de Pagamento c.Prolabore Carol.pdf\n",
            "      - Sucesso! Foram processados 40 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 04.2023 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 41 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 04.2024 ARQ - folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 68 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 05.2023 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 41 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 05.2024 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 68 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 06.2023 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 45 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 06.2024 ARQDIGITAL - Folha de Pagamento 2.pdf\n",
            "      - Sucesso! Foram processados 69 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 08.2024 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 62 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 09.2024 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 64 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10 - Extrato Mensal - 12-2023.pdf\n",
            "      - Sucesso! Foram processados 52 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10- Extrato Folha- 06-2025.pdf\n",
            "      - Sucesso! Foram processados 69 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10- Extrato Mensal 03-2023.pdf\n",
            "      - Sucesso! Foram processados 41 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10--Extrato Geral- 09-2025.pdf\n",
            "      - Sucesso! Foram processados 71 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato 13º integral.pdf\n",
            "      - Sucesso! Foram processados 50 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Decimo Terceiro- 10-2025.pdf\n",
            "      - Sucesso! Foram processados 61 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 01-2024.pdf\n",
            "      - Sucesso! Foram processados 64 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 04-2025.pdf\n",
            "      - Sucesso! Foram processados 69 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 07-2023.pdf\n",
            "      - Sucesso! Foram processados 49 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 07-2025.pdf\n",
            "      - Sucesso! Foram processados 69 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 08-2023.pdf\n",
            "      - Sucesso! Foram processados 48 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Folha- 09-2023.pdf\n",
            "      - Sucesso! Foram processados 51 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal 02-2023.pdf\n",
            "      - Sucesso! Foram processados 42 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-01-2025.pdf\n",
            "      - Sucesso! Foram processados 63 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-02-2024.pdf\n",
            "      - Sucesso! Foram processados 66 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-02-2025 ...pdf\n",
            "      - Sucesso! Foram processados 61 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-03-2024.pdf\n",
            "      - Sucesso! Foram processados 68 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-07-2024 ok.pdf\n",
            "      - Sucesso! Foram processados 67 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-08-2025.pdf\n",
            "      - Sucesso! Foram processados 68 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10-Extrato Mensal-10-2024.pdf\n",
            "      - Sucesso! Foram processados 63 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 10.2023 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 50 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 11.2023 ARQ - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 50 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 11.2024 ARQDIGITAL - 13° 1° Parcela.pdf\n",
            "      - Sucesso! Foram processados 54 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 11.2024 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 62 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 12.2024 ARQDIGITAL - Folha de Pagamento.pdf\n",
            "      - Sucesso! Foram processados 60 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: 13.2024 ARQDIGITAL - Folha Segunda Parcela do 13º Salário.pdf\n",
            "      - Sucesso! Foram processados 60 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: ARQ - 1ª Parcela 13.2023.pdf\n",
            "      - Sucesso! Foram processados 43 funcionários neste arquivo.\n",
            "\n",
            "---> Processando arquivo: Extrato Folha-05-2025.pdf\n",
            "      - Sucesso! Foram processados 68 funcionários neste arquivo.\n",
            "\n",
            "Iniciando conversão de tipos para 19 colunas...\n",
            "Convertendo coluna de data: data_admissao\n",
            "Convertendo coluna de data: data_demissao\n",
            "Convertendo coluna de data: competencia (MM/YYYY)\n",
            "Convertendo coluna monetária para Decimal: salario_contratual\n",
            "Convertendo coluna monetária para Decimal: total_proventos\n",
            "Convertendo coluna monetária para Decimal: total_descontos\n",
            "Convertendo coluna monetária para Decimal: valor_liquido\n",
            "Convertendo coluna monetária para Decimal: base_inss\n",
            "Convertendo coluna monetária para Decimal: base_fgts\n",
            "Convertendo coluna monetária para Decimal: valor_fgts\n",
            "Convertendo coluna monetária para Decimal: base_irrf\n",
            "Limpando coluna de texto candidata: competencia\n",
            "Limpando coluna de texto candidata: tipo_calculo\n",
            "Limpando coluna de texto candidata: departamento\n",
            "Limpando coluna de texto candidata: vinculo\n",
            "Limpando coluna de texto candidata: nome_funcionario\n",
            "Limpando coluna de texto candidata: situacao\n",
            "Limpando coluna de texto candidata: motivo_demissao\n",
            "Limpando coluna de texto candidata: cargo\n",
            "Limpando e padronizando coluna: cpf\n",
            "Conversão de tipos finalizada.\n",
            "\n",
            "Iniciando conversão de tipos para 10 colunas...\n",
            "Convertendo coluna de data: competencia (MM/YYYY)\n",
            "Convertendo coluna monetária para Decimal: valor_rubrica\n",
            "Limpando coluna de texto candidata: competencia\n",
            "Limpando coluna de texto candidata: tipo_calculo\n",
            "Limpando coluna de texto candidata: departamento\n",
            "Limpando coluna de texto candidata: vinculo\n",
            "Limpando coluna de texto candidata: nome_funcionario\n",
            "Limpando coluna de texto candidata: nome_rubrica\n",
            "Limpando coluna de texto candidata: tipo_rubrica\n",
            "Limpando e padronizando coluna: cpf\n",
            "Padronizando coluna: codigo_rubrica\n",
            "Conversão de tipos finalizada.\n",
            "\n",
            "--- Tipos de Dados Finais (Consolidado) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2137 entries, 0 to 2136\n",
            "Data columns (total 19 columns):\n",
            " #   Column              Non-Null Count  Dtype         \n",
            "---  ------              --------------  -----         \n",
            " 0   competencia         2137 non-null   object        \n",
            " 1   tipo_calculo        2137 non-null   object        \n",
            " 2   departamento        2094 non-null   object        \n",
            " 3   vinculo             2137 non-null   object        \n",
            " 4   nome_funcionario    2137 non-null   object        \n",
            " 5   situacao            2137 non-null   object        \n",
            " 6   data_demissao       33 non-null     datetime64[ns]\n",
            " 7   motivo_demissao     33 non-null     object        \n",
            " 8   cargo               2137 non-null   object        \n",
            " 9   data_admissao       2137 non-null   datetime64[ns]\n",
            " 10  cpf                 2137 non-null   object        \n",
            " 11  salario_contratual  2137 non-null   object        \n",
            " 12  total_proventos     2112 non-null   object        \n",
            " 13  total_descontos     2112 non-null   object        \n",
            " 14  valor_liquido       2112 non-null   object        \n",
            " 15  base_inss           2112 non-null   object        \n",
            " 16  base_fgts           2112 non-null   object        \n",
            " 17  valor_fgts          2112 non-null   object        \n",
            " 18  base_irrf           2112 non-null   object        \n",
            "dtypes: datetime64[ns](2), object(17)\n",
            "memory usage: 317.3+ KB\n",
            "\n",
            "\n",
            "--- Processo Finalizado com Sucesso! ---\n",
            "Sua base CONSOLIDADA (Totais) foi salva em: c:\\Users\\João Pedro\\Desktop\\Leitor-FOPAG\\BASE_FOPAG_CONSOLIDADA_TOTAIS.csv\n",
            "\n",
            "--- Tipos de Dados Finais (Detalhado) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13063 entries, 0 to 13062\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   competencia       13063 non-null  object\n",
            " 1   tipo_calculo      13063 non-null  object\n",
            " 2   departamento      13012 non-null  object\n",
            " 3   vinculo           13063 non-null  object\n",
            " 4   nome_funcionario  13063 non-null  object\n",
            " 5   cpf               13063 non-null  object\n",
            " 6   codigo_rubrica    13063 non-null  object\n",
            " 7   nome_rubrica      13063 non-null  object\n",
            " 8   tipo_rubrica      13063 non-null  object\n",
            " 9   valor_rubrica     13063 non-null  object\n",
            "dtypes: object(10)\n",
            "memory usage: 1020.7+ KB\n",
            "Sua base DETALHADA (Rubricas) foi salva em: c:\\Users\\João Pedro\\Desktop\\Leitor-FOPAG\\BASE_FOPAG_DETALHADA_RUBRICAS.csv\n",
            "\n",
            "Tempo total de execução: 89.61 minutos\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from decimal import Decimal, InvalidOperation # Importa o Decimal\n",
        "\n",
        "# --- MAPEAMENTO DE RUBRICAS (GLOBAL) ---\n",
        "# (Seu mapa de rubricas original - sem alterações)\n",
        "MAPEAMENTO_ORIGINAL = {\n",
        "    '12': 'P_12_13_Salario_Integral', '13': 'P_13_13_Salario_Adiantamento', '19': 'P_19_Retroativo_Salarial',\n",
        "    '22': 'P_22_Aviso_Previo', '28': 'P_28_Ferias_Vencidas', '29': 'P_29_Ferias_Proporcionais',\n",
        "    '49': 'P_49_Aviso_Previo_Nao_Trabalhado', '50': 'P_50_Adiantamento_13_Salario', '64': 'P_64_1_3_Ferias_Rescisao',\n",
        "    '150': 'P_150_Horas_Extras_50', '200': 'P_200_Horas_Extras_100', '242': 'P_242_Honorarios',\n",
        "    '246': 'P_246_Diferenca_Salarial', '250': 'P_250_Reflexo_Extra_DSR', '258': 'P_258_Anuenio_Sindpd_PA',\n",
        "    '263': 'P_263_Pag_Banco_Horas', '276': 'P_276_Trienio_Sindpd', '283': 'P_283_VT_Mes_Seguinte',\n",
        "    '295': 'P_295_Hora_Extra_50', '314': 'P_314_Dev_Desc_Indevido', '316': 'D_316_Devolucao_Desc_Plano_Odonto', # Corrected code\n",
        "    '317': 'D_317_Dev_Desc_Plano_Odonto', # Corrected code\n",
        "    '340': 'P_340_Adicional_Noturno', '399': 'P_399_Banco_Horas_Pago',\n",
        "    '461': 'P_461_Gratificacao_Funcao', '572': 'D_572_Dev_Desc_Plano_Odonto', # Corrected code\n",
        "    '574': 'P_574_Gratificacao',\n",
        "    '623': 'P_623_Gratificacao_Funcao', '695': 'P_695_Bolsa_Auxilio_Bonificacao', '700': 'P_700_Dev_Desc_INSS_Maior',\n",
        "    '725': 'P_725_Dif_Plano_Medico_Dep', '763': 'P_763_Reembolso_Conselho', '766': 'P_766_Dif_Trienio',\n",
        "    '800': 'P_800_Media_Horas_13', '801': 'P_801_Media_Valor_13', '802': 'P_802_Media_Fixa_13',\n",
        "    '803': 'P_803_13_1_12_Indenizado', '805': 'P_805_Media_Valor_Ferias', '806': 'P_806_Media_Horas_Ferias',\n",
        "    '807': 'P_807_Media_Fixa_Ferias', '808': 'P_808_Media_Valor_Abono', '809': 'P_809_Media_Horas_Abono',\n",
        "    '810': 'P_810_Media_Fixa_Abono', '811': 'P_811_Ferias_1_12_Indenizado', '817': 'P_817_Media_Fer_Proporcionais',\n",
        "    '820': 'P_820_Media_Ferias_Vencidas', '833': 'P_833_Media_Horas_13_Adiantado', '834': 'P_834_Media_Valor_13_Adiantado',\n",
        "    '835': 'P_835_Adiocional_Fixo_13_Adiantado', '836': 'P_836_Ajuste_Inss', '846': 'P_846_Dif_Abono_Ferias',\n",
        "    '854': 'P_854_Reflexo_Adicional_Noturno_DSR', '919': 'P_919_Trienio_Sinpd', '931': 'P_931_1_3_Ferias',\n",
        "    '932': 'P_932_1_3_Abono_Ferias', '940': 'P_940_Diferenca_Ferias', '995': 'P_995_Salario_Familia',\n",
        "    '1015': 'P_1015_Anuenio_Sindpd_PA', '8104': 'P_8104_13_Salario_Maternidade', '8112': 'P_8112_Dif_13_Ferias',\n",
        "    '8126': 'P_8126_1_3_Ferias_Indenizada_Resc', '8130': 'P_8130_Estouro_Rescisao', '8158': 'P_8158_Media_Ferias_1_12_Indenizado',\n",
        "    '8169': 'P_8169_1_3_Ferias_Proporcionais_Resc', '8181': 'P_8181_Dif_Media_Hora_13', '8182': 'P_8182_Dif_Media_Valor_13',\n",
        "    '8184': 'P_8184_Dif_Adicional_13', '8189': 'P_8189_Dif_Media_Horas_Ferias', '8190': 'P_8190_Dif_Media_Valor_Ferias',\n",
        "    '8192': 'P_8192_Dif_Media_Valor_Ferias', '8197': 'P_8197_Dif_Media_Horas_Abono_Ferias', '8200': 'P_8200_Dif_Adicional_Abono_Ferias',\n",
        "    '8392': 'P_8392_13_Salario_Adiantado_Ferias', '8393': 'P_8393_Media_Horas_13_Adiantado_Ferias', '8394': 'P_8394_Media_Valor_13_Adiantado_Ferias', '8156': 'P_8156_Media_Ferias_1_12_Indenizada_Resc', '8157': 'P_8157_Media_Horas_Ferias_1_12_Indenizada_Resc', '815': 'P_815_Media_Horas_Fer_Proporcional', '816': 'P_816_Media_Valor_Fer_Proporcional',\n",
        "    '8219': 'P_8219_Vantagem_13_Licenca_Maternidade', '8551': 'P_8551_Media_Horas_13_Rescisao', '8552': 'P_8552_Media_Valor_13_Rescisao',\n",
        "    '9596': 'P_9596_Media_Valor_Aviso_Previo', '9597': 'P_9597_Media_Horas_Aviso_Previo', '9600': 'P_9600_Media_Valor_1_12_Indenizado',\n",
        "    '9601': 'P_9601_Media_Horas_13_1_12_Indenizado','8396': 'P_8396_Vantagem_13_Adiantado', '8417': 'P_8417_Dif_1_3_Abono_Ferias', '8490': 'P_8490_Bolsa_Auxilio_Ferias_Proporcionais','8550': 'P_8550_13_Salario_Integral_Rescisao', '8553': 'P_8553_Media_13_Rescisao', '8781': 'P_8781_Salario_Empregado','8783': 'P_8783_Dias_Ferias', '8784': 'P_8784_Salario_Maternidade_Dias', '8791': 'P_8791_Dias_Afast_Dir_Integrais',\n",
        "    '8797': 'P_8797_Dias_Bolsa_Estagio', '8800': 'P_8800_Dias_Abono(Ferias)', '8832': 'P_8832_Dias_Licença_Maternidade',\n",
        "    '8870': 'P_8870_Dias_Afast_Doenca_Dir_Integrais', '9180': 'P_9180_Saldo_Salario_Dias', '9380': 'P_9380_Pro_Labore_Dias',\n",
        "    '9591': 'P_9591_Aviso_Previo', '9592': 'P_9592_13_1_12_Indenizado', '9598': 'P_9598_Vantagem_Aviso_Indenizado',\n",
        "    '9602': 'P_9602_Vantagem_13_1_12_Indenizado',\n",
        "    '48': 'D_48_Vale_Transporte', '51': 'D_51_Liquido_Rescisao', '241': 'D_241_Desc_Vale_Transporte',\n",
        "    '286': 'D_286_Desc_Plano_Medico_Dep', '291': 'D_291_Desc_Banco_Horas', '296': 'D_296_VT_Nao_Utilizado',\n",
        "    '297': 'D_297_VA_Nao_Utilizado', '311': 'D_311_Desc_2_Via_Cartao', '325': 'D_325_Desc_Plano_Odonto',\n",
        "    '331': 'D_331_Desc_Banco_Horas', '362': 'D_362_Desconto_VA_VR', '375': 'D_375_Desconto_Plano_Saude_Dep_F',\n",
        "    '379': 'D_379_Desconto_Plano_Odonto_F', '394': 'D_394_Desconto_Diversos', '447': 'D_447_Desc_Plano_Odonto_Alfa_Dep',\n",
        "    '449': 'D_449_Desc_Plano_Odonto_Beta', '451': 'D_451_Desc_Plano_Odonto_Alfa_Dep_F', '453': 'D_453_Desc_Plano_Odonto_Beta_F',\n",
        "    '637': 'D_637_Taxa_Campanha_Sindical', '639': 'D_639_Desconto_Valor_Pago', '777': 'D_777_VT_VA_Nao_Utilizado',\n",
        "    '804': 'D_804_IRRF_13', '812': 'D_812_INSS_Ferias', '821': 'D_821_Dif_Inss_Ferias',\n",
        "    '825': 'D_825_Inss_13_Salario', '826': 'D_826_Inss_Sobre_Rescisao', '827': 'D_827_IRRF_13_Salario_Rescisao',\n",
        "    '828': 'D_828_Irrf_Rescisao', '842': 'D_842_Multa_Estabilidade_Art_482', '843': 'D_843_Inss_Empregador',\n",
        "    '856': 'D_856_Irrf_Empregador', '858': 'D_858_INSS_Autonomo', '869': 'D_869_ISS',\n",
        "    '937': 'D_937_Adiantamento_Ferias', '942': 'D_942_Irrf_Ferias', '963': 'D_963_Desc_Odonto_Mais_Orto',\n",
        "    '964': 'D_964_Desc_Odonto_Mais_Clarear', '965': 'D_963_Desc_Odonto_Mais_Doc', '989': 'D_989_Inss_13_Sal_Rescisao',\n",
        "    '998': 'D_998_INSS', '999': 'D_999_IRRF', '1069': 'D_1069_Desc_Emprestimo_Consignado',\n",
        "    '8069': 'D_8069_Faltas_Horas_Parciais', '8111': 'D_8111_Desc_Plano_Saude_Dep', '8128': 'D_8128_IRRF_Dif_Ferias',\n",
        "    '8918': 'D_8918_Adiantamento_13_Media_Valor', '8919': 'D_8919_Adiantamento_13_Media_Horas', '8921': 'D_8921_Adiantamento_13_Media_Fixa',\n",
        "    '9750': 'D_9750_Desc_Emprestimo_Consignado', '8214': 'D_8214_INSS_Dif_13_Salario', '8215': 'D_8215_IRRF_Dif_13_Salario',\n",
        "    '8517': 'D_8517_Liquido_Rescisao_Estagiario', '8566': 'D_8566_Adiantamento_13_Salario_Rescisao'\n",
        "}\n",
        "proventos_map = {k: v for k, v in MAPEAMENTO_ORIGINAL.items() if v.startswith('P_')}\n",
        "descontos_map = {k: v for k, v in MAPEAMENTO_ORIGINAL.items() if v.startswith('D_')}\n",
        "sorted_proventos = dict(sorted(proventos_map.items(), key=lambda item: int(item[0])))\n",
        "sorted_descontos = dict(sorted(descontos_map.items(), key=lambda item: int(item[0])))\n",
        "MAPEAMENTO_CODIGOS = {**sorted_proventos, **sorted_descontos}\n",
        "# --- FIM DO MAPEAMENTO GLOBAL ---\n",
        "\n",
        "# --- [FUNÇÕES DE LIMPEZA ATUALIZADAS] ---\n",
        "\n",
        "def limpar_valor(valor_str):\n",
        "    \"\"\"\n",
        "    Converte uma string monetária para float.\n",
        "    Retorna None se o valor for nulo ou inválido.\n",
        "    \"\"\"\n",
        "    if valor_str is None:\n",
        "        return None\n",
        "    if isinstance(valor_str, (int, float)):\n",
        "        return float(valor_str)\n",
        "    if isinstance(valor_str, str):\n",
        "        try:\n",
        "            # Remove pontos de milhar, troca vírgula por ponto\n",
        "            return float(valor_str.replace('.', '').replace(',', '.'))\n",
        "        except (ValueError, TypeError):\n",
        "            # Retorna None se a string não for um número válido\n",
        "            return None\n",
        "    # Retorna None para outros tipos (ex: objetos inesperados)\n",
        "    return None\n",
        "\n",
        "def mapear_rubrica(codigo, descricao):\n",
        "    \"\"\"\n",
        "    Busca no mapa global a descrição e o tipo (Provento/Desconto) da rubrica.\n",
        "    Retorna (codigo_limpo, nome_limpo, tipo_rubrica).\n",
        "    \"\"\"\n",
        "    codigo_limpo = str(codigo).strip()\n",
        "\n",
        "    if codigo_limpo in MAPEAMENTO_CODIGOS:\n",
        "        nome_mapeado_completo = MAPEAMENTO_CODIGOS[codigo_limpo] # ex: 'P_12_13_Salario_Integral'\n",
        "        partes = nome_mapeado_completo.split('_', 2)\n",
        "        tipo_rubrica = 'Provento' if partes[0] == 'P' else 'Desconto'\n",
        "        nome_limpo = partes[2]\n",
        "        return codigo_limpo, nome_limpo, tipo_rubrica\n",
        "\n",
        "    # Fallback se não estiver no mapa\n",
        "    descricao_limpa = re.sub(r'[\\d\\s/]+$', '', descricao).strip()\n",
        "    descricao_limpa = re.sub(r'\\s+', '_', descricao_limpa).upper()\n",
        "    nome_fallback = f\"NAO_MAPEADO_{descricao_limpa}\"\n",
        "    # MODIFICADO: Retorna None ao invés de 'N/A'\n",
        "    return codigo_limpo, nome_fallback, None\n",
        "\n",
        "\n",
        "def extrair_info_base(texto_pagina):\n",
        "    \"\"\"Extrai a competência e o tipo de cálculo do documento.\"\"\"\n",
        "    competencia_match = re.search(r'Competência:\\s*(\\d{2}/\\d{4})', texto_pagina)\n",
        "    calculo_match = re.search(r'Cálculo\\s*:\\s*(.+)', texto_pagina)\n",
        "    return {\n",
        "        # MODIFICADO: Usa 'None' como padrão\n",
        "        'competencia': competencia_match.group(1).strip() if competencia_match else None,\n",
        "        'tipo_calculo': calculo_match.group(1).strip() if calculo_match else None\n",
        "    }\n",
        "\n",
        "# --- [NOVA FUNÇÃO DE TRATAMENTO DE TIPOS] ---\n",
        "\n",
        "def converter_para_decimal(valor):\n",
        "    \"\"\"Função auxiliar para converter valores (float ou str) para Decimal.\"\"\"\n",
        "    if valor is None or pd.isna(valor):\n",
        "        return None\n",
        "    try:\n",
        "        # Converte o valor (que é float) para string e depois para Decimal\n",
        "        # Isso evita problemas de precisão do float (ex: 10.1 vira 10.09999)\n",
        "        return Decimal(str(valor))\n",
        "    except (InvalidOperation, ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def tratar_tipos_para_postgres(df):\n",
        "    \"\"\"\n",
        "    Converte as colunas de um DataFrame para os tipos corretos (datetime, Decimal, string)\n",
        "    ANTES de enviar ao Postgres.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    print(f\"\\nIniciando conversão de tipos para {df.shape[1]} colunas...\")\n",
        "\n",
        "    # --- COLUNAS DE DATA (Formato DD/MM/YYYY) ---\n",
        "    colunas_data = ['data_admissao', 'data_demissao']\n",
        "    for col in colunas_data:\n",
        "        if col in df.columns:\n",
        "            print(f\"Convertendo coluna de data: {col}\")\n",
        "            # errors='coerce' transforma datas inválidas (or None) em NaT (Not a Time)\n",
        "            df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='coerce', dayfirst=False)\n",
        "\n",
        "    # --- COLUNA DE COMPETÊNCIA (Formato especial MM/YYYY) ---\n",
        "    if 'competencia' in df.columns:\n",
        "        print(\"Convertendo coluna de data: competencia (MM/YYYY)\")\n",
        "        # Adiciona '01/' para criar uma data completa e válida\n",
        "        # errors='coerce' trata valores nulos (None) ou formatos ruins\n",
        "        df['competencia'] = pd.to_datetime('01/' + df['competencia'].astype(str), format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "    # --- COLUNAS MONETÁRIAS (Para NUMERIC/Decimal) ---\n",
        "    colunas_monetarias = [\n",
        "        'salario_contratual', 'total_proventos', 'total_descontos',\n",
        "        'valor_liquido', 'base_inss', 'base_fgts', 'valor_fgts',\n",
        "        'base_irrf', 'valor_rubrica'\n",
        "    ]\n",
        "    for col in colunas_monetarias:\n",
        "        if col in df.columns:\n",
        "            print(f\"Convertendo coluna monetária para Decimal: {col}\")\n",
        "            # A função limpar_valor já retornou float.\n",
        "            # Agora convertemos de float para Decimal de forma segura.\n",
        "            df[col] = df[col].apply(converter_para_decimal)\n",
        "\n",
        "    # --- COLUNAS DE TEXTO (String/Object) ---\n",
        "    # Limpa espaços em branco e padroniza nulos\n",
        "    # Consider columns that *should* be objects\n",
        "    colunas_texto_candidatas = [col for col in df.columns if col not in colunas_data + colunas_monetarias + ['cpf', 'codigo_rubrica']]\n",
        "\n",
        "    for col in colunas_texto_candidatas:\n",
        "        if col in df.columns:\n",
        "            print(f\"Limpando coluna de texto candidata: {col}\")\n",
        "            # Attempt conversion to string and then apply strip/replace\n",
        "            try:\n",
        "                df[col] = df[col].astype(str).str.strip()\n",
        "                # Replace 'None' string created by astype(str) for actual None values\n",
        "                df[col] = df[col].replace('None', None)\n",
        "                # Substitui strings vazias por None (que vira NULL)\n",
        "                df[col] = df[col].replace('', None)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not process column '{col}' as string. Error: {e}\")\n",
        "                # If conversion or string operations fail, leave the column as is or handle differently\n",
        "                pass\n",
        "\n",
        "\n",
        "    # Tratamento especial para CPF (remover formatação)\n",
        "    if 'cpf' in df.columns:\n",
        "         print(\"Limpando e padronizando coluna: cpf\")\n",
        "         # Ensure it's string before applying replace\n",
        "         df['cpf'] = df['cpf'].astype(str).str.replace(r'[^\\d]', '', regex=True).replace('', None)\n",
        "         df['cpf'] = df['cpf'].replace('None', None) # Replace 'None' string if any\n",
        "\n",
        "    # Garante que codigo_rubrica seja texto\n",
        "    if 'codigo_rubrica' in df.columns:\n",
        "         print(\"Padronizando coluna: codigo_rubrica\")\n",
        "         # Ensure it's treated as string before replacing 'None'\n",
        "         df['codigo_rubrica'] = df['codigo_rubrica'].astype(str).replace('None', None)\n",
        "\n",
        "    print(\"Conversão de tipos finalizada.\")\n",
        "    return df\n",
        "\n",
        "# --- [FUNÇÃO PRINCIPAL MODIFICADA] ---\n",
        "\n",
        "def processar_pdfs_na_pasta(pasta_path):\n",
        "    \"\"\"\n",
        "    Função principal que varre uma pasta, processa todos os PDFs e\n",
        "    retorna DOIS DataFrames: um consolidado (totais) e um detalhado (rubricas).\n",
        "    \"\"\"\n",
        "    arquivos_pdf = [f for f in os.listdir(pasta_path) if f.lower().endswith('.pdf')]\n",
        "    if not arquivos_pdf:\n",
        "        print(f\"Nenhum arquivo PDF encontrado na pasta: {pasta_path}\")\n",
        "        return None, None\n",
        "\n",
        "    lista_geral_rubricas_detalhadas = []\n",
        "    lista_geral_consolidados = []\n",
        "\n",
        "    print(f\"Encontrados {len(arquivos_pdf)} PDFs para processar...\")\n",
        "\n",
        "    for nome_arquivo in arquivos_pdf:\n",
        "        print(f\"\\n---> Processando arquivo: {nome_arquivo}\")\n",
        "        try:\n",
        "            with pdfplumber.open(os.path.join(pasta_path, nome_arquivo)) as pdf:\n",
        "                texto_completo_pdf = \"\".join([(page.extract_text(x_tolerance=1, y_tolerance=1) or \"\") + \"\\n\" for page in pdf.pages])\n",
        "                info_base = extrair_info_base(texto_completo_pdf)\n",
        "                depto_map = {match.start(): match.group(1).strip() for match in re.finditer(r'Departamento:\\s*(.+)', texto_completo_pdf)}\n",
        "                depto_indices = sorted(depto_map.keys())\n",
        "                blocos_encontrados = re.finditer(r'((?:Empr|Contr)\\.?\\s*:\\s*\\d+.*?)(?=\\n(?:Empr|Contr)\\.?\\s*:\\s*\\d+|Resumo por Rubricas|Totais por Departamento)', texto_completo_pdf, re.DOTALL)\n",
        "\n",
        "                funcionarios_processados_no_arquivo = 0\n",
        "                for bloco_match in blocos_encontrados:\n",
        "                    bloco = bloco_match.group(1)\n",
        "\n",
        "                    if \"CPF:\" not in bloco: continue\n",
        "\n",
        "                    posicao_bloco = bloco_match.start()\n",
        "                    # MODIFICADO: Usa 'None' como padrão\n",
        "                    departamento_atual = next((depto_map[idx] for idx in reversed(depto_indices) if idx < posicao_bloco), None)\n",
        "\n",
        "                    dados_funcionario = {'departamento': departamento_atual, **info_base}\n",
        "\n",
        "                    vinculo_match = re.search(r'(Empr|Contr)\\.?', bloco)\n",
        "                    # MODIFICADO: Usa 'None' como padrão\n",
        "                    dados_funcionario['vinculo'] = 'Empregado' if vinculo_match and 'Empr' in vinculo_match.group(0) else 'Contribuinte' if vinculo_match else None\n",
        "\n",
        "                    situacao_match = re.search(r'Situação:\\s*([^\\n\\r]+)', bloco)\n",
        "                    if situacao_match:\n",
        "                        situacao_str = re.split(r'\\s+CPF:|\\s+Adm:', situacao_match.group(1))[0].strip()\n",
        "                        dados_funcionario['situacao'] = situacao_str\n",
        "                    else:\n",
        "                        header_chunk_match = re.search(r'(?:Empr|Contr)\\.?\\s*:\\s*\\d+.*?(?=\\n|CPF:)', bloco, re.DOTALL)\n",
        "                        if header_chunk_match:\n",
        "                            header_chunk = header_chunk_match.group(0)\n",
        "                            unlabeled_status_match = re.search(r'\\s(Trabalhando|Afastado|Férias|Demitido)\\s*$', header_chunk, re.IGNORECASE)\n",
        "                            if unlabeled_status_match:\n",
        "                                dados_funcionario['situacao'] = unlabeled_status_match.group(1)\n",
        "                            else:\n",
        "                                dados_funcionario['situacao'] = None # MODIFICADO\n",
        "                        else:\n",
        "                             dados_funcionario['situacao'] = None # MODIFICADO\n",
        "\n",
        "                    demissao_motivo_match = re.search(r'DEMITIDO EM\\s+(\\d{2}/\\d{2}/\\d{4})\\s*-\\s*(.*?)(?=\\n|$)', bloco, re.IGNORECASE | re.DOTALL)\n",
        "                    if demissao_motivo_match:\n",
        "                        dados_funcionario['data_demissao'] = demissao_motivo_match.group(1).strip()\n",
        "                        dados_funcionario['motivo_demissao'] = demissao_motivo_match.group(2).strip()\n",
        "                    else:\n",
        "                        demissao_match_antigo = re.search(r'(?:Data Demissão|Demissão):\\s*(\\d{2}/\\d{2}/\\d{4})', bloco, re.IGNORECASE)\n",
        "                        # MODIFICADO: Usa 'None' como padrão\n",
        "                        dados_funcionario['data_demissao'] = demissao_match_antigo.group(1).strip() if demissao_match_antigo else None\n",
        "                        dados_funcionario['motivo_demissao'] = None\n",
        "\n",
        "                    delimitadores_nome = r'(?=\\s*Situação:|\\s*CPF:|\\s*Adm:|\\n)'\n",
        "                    regex_nome = r'(?:Empr|Contr)\\.?\\s*:\\s*\\d+\\s+(.*?)' + delimitadores_nome\n",
        "                    nome_match = re.search(regex_nome, bloco, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "                    if nome_match:\n",
        "                        nome_capturado = nome_match.group(1).replace('\\n', ' ').strip()\n",
        "                        status_encontrado = dados_funcionario.get('situacao', None) # MODIFICADO\n",
        "                        nome_limpo = nome_capturado\n",
        "                        if status_encontrado != None and nome_limpo.lower().endswith(status_encontrado.lower()):\n",
        "                            tamanho_status = len(status_encontrado)\n",
        "                            nome_limpo = nome_limpo[:-tamanho_status].strip()\n",
        "                        nome_limpo = re.sub(r'[^\\s]+:\\s*$', '', nome_limpo).strip()\n",
        "                        dados_funcionario['nome_funcionario'] = nome_limpo\n",
        "                    else:\n",
        "                        dados_funcionario['nome_funcionario'] = None # MODIFICADO\n",
        "\n",
        "                    cpf_match = re.search(r'CPF:\\s*([\\d\\.\\-]+)', bloco)\n",
        "                    # MODIFICADO: Usa 'None' como padrão\n",
        "                    dados_funcionario['cpf'] = cpf_match.group(1).strip() if cpf_match else None\n",
        "                    admissao_match = re.search(r'Adm?:\\s*(\\d{2}/\\d{2}/\\d{4})', bloco)\n",
        "                    dados_funcionario['data_admissao'] = admissao_match.group(1).strip() if admissao_match else None # MODIFICADO\n",
        "                    cargo_match = re.search(r'Cargo:\\s*\\d+\\s+(.*?)(?=\\s+Salário:|\\s+C\\.|С\\.)', bloco, re.DOTALL)\n",
        "                    dados_funcionario['cargo'] = cargo_match.group(1).replace('\\n', ' ').strip() if cargo_match else None # MODIFICADO\n",
        "\n",
        "                    # MODIFICADO: Salário já usa limpar_valor, que retorna float ou None\n",
        "                    salario_match = re.search(r'Salário:\\s*([\\d\\.,]+)', bloco)\n",
        "                    dados_funcionario['salario_contratual'] = limpar_valor(salario_match.group(1)) if salario_match else None\n",
        "\n",
        "                    rodape_bloco = bloco[bloco.find(\"ND:\"):] if \"ND:\" in bloco else \"\"\n",
        "\n",
        "                    rodape_match = re.search(r'Proventos:\\s*([\\d\\.,]+)\\s+Descontos:\\s*([\\d\\.,]+).*?L[íi]quido:\\s*([\\d\\.,]+).*?Base INSS:\\s*([\\d\\.,]+).*?Base FGTS:\\s*([\\d\\.,]+).*?Valor FGTS:\\s*([\\d\\.,]+).*?Base IRRF:\\s*([\\d\\.,]+)', rodape_bloco, re.DOTALL)\n",
        "\n",
        "                    if rodape_match:\n",
        "                        dados_funcionario.update({\n",
        "                            'total_proventos': limpar_valor(rodape_match.group(1)), 'total_descontos': limpar_valor(rodape_match.group(2)),\n",
        "                            'valor_liquido': limpar_valor(rodape_match.group(3)), 'base_inss': limpar_valor(rodape_match.group(4)),\n",
        "                            'base_fgts': limpar_valor(rodape_match.group(5)), 'valor_fgts': limpar_valor(rodape_match.group(6)),\n",
        "                            'base_irrf': limpar_valor(rodape_match.group(7))\n",
        "                        })\n",
        "                    # MODIFICADO: Adiciona 'else' para garantir que as colunas existam (com None)\n",
        "                    else:\n",
        "                        dados_funcionario.update({\n",
        "                            'total_proventos': None, 'total_descontos': None, 'valor_liquido': None,\n",
        "                            'base_inss': None, 'base_fgts': None, 'valor_fgts': None, 'base_irrf': None\n",
        "                        })\n",
        "\n",
        "                    # 1. Salva a linha CONSOLIDADA\n",
        "                    lista_geral_consolidados.append(dados_funcionario.copy())\n",
        "\n",
        "                    # 2. Define as CHAVES para a tabela detalhada\n",
        "                    chaves_funcionario_para_rubricas = {\n",
        "                        'competencia': dados_funcionario.get('competencia'),\n",
        "                        'tipo_calculo': dados_funcionario.get('tipo_calculo'),\n",
        "                        'departamento': dados_funcionario.get('departamento'),\n",
        "                        'vinculo': dados_funcionario.get('vinculo'),\n",
        "                        'nome_funcionario': dados_funcionario.get('nome_funcionario'),\n",
        "                        'cpf': dados_funcionario.get('cpf')\n",
        "                    }\n",
        "\n",
        "                    # 3. Processa as RUBRICAS\n",
        "                    rubricas_extraidas = []\n",
        "                    inicio_tabela = max(bloco.find(\"CPF:\"), bloco.find(\"CPF:\"))\n",
        "                    fim_tabela = bloco.find(\"\\nND:\")\n",
        "\n",
        "                    if inicio_tabela != -1 and fim_tabela != -1:\n",
        "                        tabela_str = bloco[inicio_tabela:fim_tabela].split('\\n')[1:]\n",
        "                        for linha in tabela_str:\n",
        "                            if not re.search(r'\\d', linha): continue\n",
        "                            padrao_verba = r'(\\d+)\\s+(.*?)\\s+([\\d\\.,]+)\\s+([PD])(?=\\s+\\d{2,}|$)'\n",
        "                            matches = re.finditer(padrao_verba, linha)\n",
        "                            for match in matches:\n",
        "                                valor_limpo = limpar_valor(match.group(3))\n",
        "                                if valor_limpo == 0: continue\n",
        "                                codigo_bruto = match.group(1)\n",
        "                                descricao_bruta = match.group(2).strip()\n",
        "                                descricao_bruta = re.sub(r'\\s[\\d\\.,%]+$', '', descricao_bruta).strip()\n",
        "                                (codigo_limpo, nome_mapeado, tipo_rubrica) = mapear_rubrica(codigo_bruto, descricao_bruta)\n",
        "                                rubricas_extraidas.append({\n",
        "                                    'codigo_rubrica': codigo_limpo,\n",
        "                                    'nome_rubrica': nome_mapeado,\n",
        "                                    'tipo_rubrica': tipo_rubrica,\n",
        "                                    'valor_rubrica': valor_limpo\n",
        "                                })\n",
        "\n",
        "                    # 4. Adiciona as rubricas à lista DETALHADA\n",
        "                    if rubricas_extraidas:\n",
        "                        for rubrica in rubricas_extraidas:\n",
        "                            nova_linha_longa = chaves_funcionario_para_rubricas.copy()\n",
        "                            nova_linha_longa.update(rubrica)\n",
        "                            lista_geral_rubricas_detalhadas.append(nova_linha_longa)\n",
        "                    else:\n",
        "                        # MODIFICADO: Preenche com None e 0.0 para manter tipos\n",
        "                        linha_sem_rubrica = chaves_funcionario_para_rubricas.copy()\n",
        "                        linha_sem_rubrica.update({\n",
        "                            'codigo_rubrica': None,\n",
        "                            'nome_rubrica': None,\n",
        "                            'tipo_rubrica': None,\n",
        "                            'valor_rubrica': 0.0 # Zero é aceitável para \"sem rubrica\"\n",
        "                        })\n",
        "                        lista_geral_rubricas_detalhadas.append(linha_sem_rubrica)\n",
        "\n",
        "                    funcionarios_processados_no_arquivo += 1\n",
        "\n",
        "            print(f\"      - Sucesso! Foram processados {funcionarios_processados_no_arquivo} funcionários neste arquivo.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ERRO CRÍTICO ao processar o arquivo {nome_arquivo}: {e}\")\n",
        "\n",
        "    if not lista_geral_consolidados and not lista_geral_rubricas_detalhadas:\n",
        "        print(\"\\nProcesso concluído, mas nenhum dado pôde ser extraído.\")\n",
        "        return None, None\n",
        "\n",
        "    # --- [CRIAÇÃO E TRATAMENTO DOS DATAFRAMES FINAIS] ---\n",
        "\n",
        "    # 1. DataFrame CONSOLIDADO (Totais)\n",
        "    df_consolidado = pd.DataFrame(lista_geral_consolidados)\n",
        "    colunas_info_pessoal = [\n",
        "        'competencia', 'tipo_calculo', 'departamento', 'vinculo', 'nome_funcionario',\n",
        "        'situacao', 'data_demissao', 'motivo_demissao', 'cargo', 'data_admissao', 'cpf',\n",
        "        'salario_contratual', 'total_proventos', 'total_descontos', 'valor_liquido', 'base_inss', 'base_fgts',\n",
        "        'valor_fgts', 'base_irrf'\n",
        "    ]\n",
        "    colunas_presentes_consol = [col for col in colunas_info_pessoal if col in df_consolidado.columns]\n",
        "    if not df_consolidado.empty:\n",
        "        df_consolidado = df_consolidado[colunas_presentes_consol]\n",
        "\n",
        "    # 2. DataFrame DETALHADO (Rubricas)\n",
        "    df_detalhado = pd.DataFrame(lista_geral_rubricas_detalhadas)\n",
        "    colunas_chaves = [\n",
        "         'competencia', 'tipo_calculo', 'departamento', 'vinculo', 'nome_funcionario', 'cpf'\n",
        "    ]\n",
        "    colunas_rubrica_longa = [\n",
        "        'codigo_rubrica', 'nome_rubrica', 'tipo_rubrica', 'valor_rubrica'\n",
        "    ]\n",
        "    colunas_presentes_chaves = [col for col in colunas_chaves if col in df_detalhado.columns]\n",
        "    colunas_presentes_rubricas = [col for col in colunas_rubrica_longa if col in df_detalhado.columns]\n",
        "    ordem_final_detalhada = colunas_presentes_chaves + colunas_presentes_rubricas\n",
        "    if not df_detalhado.empty:\n",
        "        df_detalhado = df_detalhado[ordem_final_detalhada]\n",
        "\n",
        "    # --- [REMOÇÃO DA FUNÇÃO 'preencher_nulos'] ---\n",
        "    # Não vamos mais preencher nulos com 'N/A' ou 0.0.\n",
        "    # O valor 'None' (que vira pd.NA ou pd.NaT) é o correto para o banco (NULL).\n",
        "\n",
        "    # --- [ETAPA FINAL DE CONVERSÃO DE TIPOS] ---\n",
        "    df_consolidado_tratado = tratar_tipos_para_postgres(df_consolidado)\n",
        "    df_detalhado_tratado = tratar_tipos_para_postgres(df_detalhado)\n",
        "\n",
        "    return df_detalhado_tratado, df_consolidado_tratado\n",
        "\n",
        "# --- PONTO DE EXECUÇÃO ---\n",
        "if __name__ == \"__main__\":\n",
        "    inicio_tempo = time.perf_counter()\n",
        "    # ATENÇÃO: Coloque o caminho correto para sua pasta de PDFs\n",
        "    caminho_da_pasta = r'C:\\Users\\João Pedro\\Desktop\\Leitor-FOPAG\\FOPAG' # Exemplo para Google Colab\n",
        "    # Em ambiente local, seria algo como:\n",
        "    # caminho_da_pasta = r'C:\\Meus Documentos\\Folha Pagamento\\PDFs'\n",
        "\n",
        "    # Os dataframes retornados agora estão com os tipos corretos\n",
        "    df_detalhado_final, df_consolidado_final = processar_pdfs_na_pasta(caminho_da_pasta)\n",
        "\n",
        "    if df_consolidado_final is not None and not df_consolidado_final.empty:\n",
        "        # Mostra os tipos de dados (dtypes) do DataFrame final\n",
        "        print(\"\\n--- Tipos de Dados Finais (Consolidado) ---\")\n",
        "        df_consolidado_final.info()\n",
        "\n",
        "        nome_arquivo_saida_consol = 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv'\n",
        "        # Salva em CSV (para manter seu processo original)\n",
        "        df_consolidado_final.to_csv(nome_arquivo_saida_consol, index=False, sep=';', decimal=',', encoding='utf-8-sig')\n",
        "        print(\"\\n\\n--- Processo Finalizado com Sucesso! ---\")\n",
        "        print(f\"Sua base CONSOLIDADA (Totais) foi salva em: {os.path.abspath(nome_arquivo_saida_consol)}\")\n",
        "    else:\n",
        "        print(\"\\nNenhum dado CONSOLIDADO foi gerado.\")\n",
        "\n",
        "    if df_detalhado_final is not None and not df_detalhado_final.empty:\n",
        "        # Mostra os tipos de dados (dtypes) do DataFrame final\n",
        "        print(\"\\n--- Tipos de Dados Finais (Detalhado) ---\")\n",
        "        df_detalhado_final.info()\n",
        "\n",
        "        nome_arquivo_saida_detal = 'BASE_FOPAG_DETALHADA_RUBRICAS.csv'\n",
        "        # Salva em CSV\n",
        "        df_detalhado_final.to_csv(nome_arquivo_saida_detal, index=False, sep=';', decimal=',', encoding='utf-8-sig')\n",
        "        print(f\"Sua base DETALHADA (Rubricas) foi salva em: {os.path.abspath(nome_arquivo_saida_detal)}\")\n",
        "    else:\n",
        "        print(\"Nenhum dado DETALHADO de rubricas foi gerado.\")\n",
        "\n",
        "    if df_consolidado_final is None and df_detalhado_final is None:\n",
        "         print(\"\\nNenhum dado foi gerado. Verifique se os PDFs estão na pasta correta e não estão corrompidos.\")\n",
        "\n",
        "    fim_tempo = time.perf_counter()\n",
        "    duracao = (fim_tempo - inicio_tempo)\n",
        "    print(f\"\\nTempo total de execução: {duracao:.2f} minutos\") # Mudei para segundos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d72e248f",
      "metadata": {
        "id": "d72e248f",
        "outputId": "dd0f62b7-7a2a-407b-e679-d49da709f448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2137 entries, 0 to 2136\n",
            "Data columns (total 19 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   competencia         2137 non-null   object \n",
            " 1   tipo_calculo        2137 non-null   object \n",
            " 2   departamento        2094 non-null   object \n",
            " 3   vinculo             2137 non-null   object \n",
            " 4   nome_funcionario    2137 non-null   object \n",
            " 5   situacao            2137 non-null   object \n",
            " 6   data_demissao       33 non-null     object \n",
            " 7   motivo_demissao     33 non-null     object \n",
            " 8   cargo               2137 non-null   object \n",
            " 9   data_admissao       2137 non-null   object \n",
            " 10  cpf                 2137 non-null   int64  \n",
            " 11  salario_contratual  2137 non-null   float64\n",
            " 12  total_proventos     2112 non-null   float64\n",
            " 13  total_descontos     2112 non-null   float64\n",
            " 14  valor_liquido       2112 non-null   float64\n",
            " 15  base_inss           2112 non-null   float64\n",
            " 16  base_fgts           2112 non-null   float64\n",
            " 17  valor_fgts          2112 non-null   float64\n",
            " 18  base_irrf           2112 non-null   float64\n",
            "dtypes: float64(8), int64(1), object(10)\n",
            "memory usage: 317.3+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(r'C:\\Users\\João Pedro\\Desktop\\Leitor-FOPAG\\BASE_FOPAG_CONSOLIDADA_TOTAIS.csv',sep=';')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1220f609",
      "metadata": {
        "id": "1220f609",
        "outputId": "8bea95c5-54e2-43f2-a703-5b78cf3ff895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando processo de carga INCREMENTAL da FOPAG para o Postgres...\n",
            "Conexão com o banco 'postgres' estabelecida com sucesso.\n",
            "Schema 'FOPAG_Long' verificado/criado com sucesso.\n",
            "\n",
            "--- Processando arquivo: BASE_FOPAG_CONSOLIDADA_TOTAIS.csv ---\n",
            "Arquivo lido com sucesso (como texto). 2137 linhas carregadas.\n",
            "--- Amostra (head) de BASE_FOPAG_CONSOLIDADA_TOTAIS.csv (dados crus):\n",
            "  competencia                                 tipo_calculo  \\\n",
            "0  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "1  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "2  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "3  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "4  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "\n",
            "                           departamento       vinculo  \\\n",
            "0  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "1  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "2  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "3               3 - ARQDIGITAL BRASILIA     Empregado   \n",
            "4               3 - ARQDIGITAL BRASILIA     Empregado   \n",
            "\n",
            "               nome_funcionario     situacao data_demissao motivo_demissao  \\\n",
            "0      CAROLINE BARRETO MOREIRA  Trabalhando           NaN             NaN   \n",
            "1  LIDIANE SILVA BOLINA CARRIAO  Trabalhando           NaN             NaN   \n",
            "2    RICARDO DA COSTA BROCKVELD  Trabalhando           NaN             NaN   \n",
            "3         ALINE ALMEIDA BARBOSA  Trabalhando           NaN             NaN   \n",
            "4          CAMILA LINS DE ANJOS  Trabalhando           NaN             NaN   \n",
            "\n",
            "                                   cargo data_admissao          cpf  \\\n",
            "0  DIRETOR(A) DE COMUNICAÇÃO E MARKETING    2023-01-02  79445632168   \n",
            "1                    DIRETOR(A) JURIDICO    2018-08-01  02506367195   \n",
            "2                 DIRETOR(A) CORPORATIVO    2018-08-01  79580670110   \n",
            "3                       LIDER FINANCEIRO    2013-08-05  02453194178   \n",
            "4                         JOVEM APRENDIZ    2021-05-03  08028394108   \n",
            "\n",
            "  salario_contratual total_proventos total_descontos valor_liquido base_inss  \\\n",
            "0            10000.0         9677.42         2390.65       7286.77   7507.49   \n",
            "1            17957.0         17957.0          4615.4       13341.6   7507.49   \n",
            "2            17957.0         17957.0         4541.64      13415.36   7507.49   \n",
            "3             5575.0          7252.9         5062.44       2190.46    7252.9   \n",
            "4              785.0           785.0           58.87        726.13     785.0   \n",
            "\n",
            "  base_fgts valor_fgts base_irrf  \n",
            "0       0.0        0.0    8851.6  \n",
            "1       0.0        0.0  16941.59  \n",
            "2       0.0        0.0  16562.41  \n",
            "3    7252.9     580.22   2011.95  \n",
            "4     785.0       15.7    726.13  \n",
            "--- Info de BASE_FOPAG_CONSOLIDADA_TOTAIS.csv (dados crus):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2137 entries, 0 to 2136\n",
            "Data columns (total 19 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   competencia         2137 non-null   object\n",
            " 1   tipo_calculo        2137 non-null   object\n",
            " 2   departamento        2094 non-null   object\n",
            " 3   vinculo             2137 non-null   object\n",
            " 4   nome_funcionario    2137 non-null   object\n",
            " 5   situacao            2137 non-null   object\n",
            " 6   data_demissao       33 non-null     object\n",
            " 7   motivo_demissao     33 non-null     object\n",
            " 8   cargo               2137 non-null   object\n",
            " 9   data_admissao       2137 non-null   object\n",
            " 10  cpf                 2137 non-null   object\n",
            " 11  salario_contratual  2137 non-null   object\n",
            " 12  total_proventos     2112 non-null   object\n",
            " 13  total_descontos     2112 non-null   object\n",
            " 14  valor_liquido       2112 non-null   object\n",
            " 15  base_inss           2112 non-null   object\n",
            " 16  base_fgts           2112 non-null   object\n",
            " 17  valor_fgts          2112 non-null   object\n",
            " 18  base_irrf           2112 non-null   object\n",
            "dtypes: object(19)\n",
            "memory usage: 317.3+ KB\n",
            "Iniciando tratamento de tipos para BASE_FOPAG_CONSOLIDADA_TOTAIS.csv...\n",
            "Tratando tipo de data: competencia\n",
            "  -> Formato com '-' detectado. Usando parser nativo (YYYY-MM-DD).\n",
            "Tratando tipo de data: data_admissao\n",
            "  -> Formato com '-' detectado. Usando parser nativo (YYYY-MM-DD).\n",
            "Tratando tipo de data: data_demissao\n",
            "  -> Formato com '-' detectado. Usando parser nativo (YYYY-MM-DD).\n",
            "Tratando tipo: salario_contratual (String -> Decimal)\n",
            "Tratando tipo: total_proventos (String -> Decimal)\n",
            "Tratando tipo: total_descontos (String -> Decimal)\n",
            "Tratando tipo: valor_liquido (String -> Decimal)\n",
            "Tratando tipo: base_inss (String -> Decimal)\n",
            "Tratando tipo: base_fgts (String -> Decimal)\n",
            "Tratando tipo: valor_fgts (String -> Decimal)\n",
            "Tratando tipo: base_irrf (String -> Decimal)\n",
            "Tratando tipo: cpf (String -> String Limpa)\n",
            "Limpando coluna de texto: tipo_calculo\n",
            "Limpando coluna de texto: departamento\n",
            "Limpando coluna de texto: vinculo\n",
            "Limpando coluna de texto: nome_funcionario\n",
            "Limpando coluna de texto: situacao\n",
            "Limpando coluna de texto: motivo_demissao\n",
            "Limpando coluna de texto: cargo\n",
            "Tratamento de tipos finalizado.\n",
            "--- Info de BASE_FOPAG_CONSOLIDADA_TOTAIS.csv (APÓS tratamento):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2137 entries, 0 to 2136\n",
            "Data columns (total 19 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   competencia         2137 non-null   object\n",
            " 1   tipo_calculo        2137 non-null   object\n",
            " 2   departamento        2094 non-null   object\n",
            " 3   vinculo             2137 non-null   object\n",
            " 4   nome_funcionario    2137 non-null   object\n",
            " 5   situacao            2137 non-null   object\n",
            " 6   data_demissao       33 non-null     object\n",
            " 7   motivo_demissao     33 non-null     object\n",
            " 8   cargo               2137 non-null   object\n",
            " 9   data_admissao       2137 non-null   object\n",
            " 10  cpf                 2137 non-null   object\n",
            " 11  salario_contratual  2137 non-null   object\n",
            " 12  total_proventos     2112 non-null   object\n",
            " 13  total_descontos     2112 non-null   object\n",
            " 14  valor_liquido       2112 non-null   object\n",
            " 15  base_inss           2112 non-null   object\n",
            " 16  base_fgts           2112 non-null   object\n",
            " 17  valor_fgts          2112 non-null   object\n",
            " 18  base_irrf           2112 non-null   object\n",
            "dtypes: object(19)\n",
            "memory usage: 317.3+ KB\n",
            "Competências a serem carregadas: [datetime.date(2023, 1, 1) datetime.date(2023, 4, 1)\n",
            " datetime.date(2024, 4, 1) datetime.date(2023, 5, 1)\n",
            " datetime.date(2024, 5, 1) datetime.date(2023, 6, 1)\n",
            " datetime.date(2024, 6, 1) datetime.date(2024, 8, 1)\n",
            " datetime.date(2024, 9, 1) datetime.date(2023, 12, 1)\n",
            " datetime.date(2025, 6, 1) datetime.date(2023, 3, 1)\n",
            " datetime.date(2025, 9, 1) datetime.date(2025, 10, 1)\n",
            " datetime.date(2024, 1, 1) datetime.date(2025, 4, 1)\n",
            " datetime.date(2023, 7, 1) datetime.date(2025, 7, 1)\n",
            " datetime.date(2023, 8, 1) datetime.date(2023, 9, 1)\n",
            " datetime.date(2023, 2, 1) datetime.date(2025, 1, 1)\n",
            " datetime.date(2024, 2, 1) datetime.date(2025, 2, 1)\n",
            " datetime.date(2024, 3, 1) datetime.date(2024, 7, 1)\n",
            " datetime.date(2025, 8, 1) datetime.date(2024, 10, 1)\n",
            " datetime.date(2023, 10, 1) datetime.date(2023, 11, 1)\n",
            " datetime.date(2024, 11, 1) datetime.date(2024, 12, 1)\n",
            " datetime.date(2025, 5, 1)]\n",
            "Tabela '\"FOPAG_Long\".\"fopag_totais\"' não existe. Criando e inserindo (primeira carga)...\n",
            "SUCESSO! Tabela '\"FOPAG_Long\".\"fopag_totais\"' criada com tipos corretos e dados carregados.\n",
            "\n",
            "--- Processando arquivo: BASE_FOPAG_DETALHADA_RUBRICAS.csv ---\n",
            "Arquivo lido com sucesso (como texto). 13063 linhas carregadas.\n",
            "--- Amostra (head) de BASE_FOPAG_DETALHADA_RUBRICAS.csv (dados crus):\n",
            "  competencia                                 tipo_calculo  \\\n",
            "0  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "1  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "2  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "3  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "4  2023-01-01  Folha Mensal e Complementar Horas: 20:50:15   \n",
            "\n",
            "                           departamento       vinculo  \\\n",
            "0  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "1  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "2  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "3  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "4  2 - ARQDIGITAL ADMINISTRAÇÃO CENTRAL  Contribuinte   \n",
            "\n",
            "               nome_funcionario          cpf codigo_rubrica     nome_rubrica  \\\n",
            "0      CAROLINE BARRETO MOREIRA  79445632168           9380  Pro_Labore_Dias   \n",
            "1      CAROLINE BARRETO MOREIRA  79445632168            843  Inss_Empregador   \n",
            "2      CAROLINE BARRETO MOREIRA  79445632168            856  Irrf_Empregador   \n",
            "3  LIDIANE SILVA BOLINA CARRIAO  02506367195           9380  Pro_Labore_Dias   \n",
            "4  LIDIANE SILVA BOLINA CARRIAO  02506367195            843  Inss_Empregador   \n",
            "\n",
            "  tipo_rubrica valor_rubrica  \n",
            "0     Provento       9677.42  \n",
            "1     Desconto        825.82  \n",
            "2     Desconto       1564.83  \n",
            "3     Provento       17957.0  \n",
            "4     Desconto        825.82  \n",
            "--- Info de BASE_FOPAG_DETALHADA_RUBRICAS.csv (dados crus):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13063 entries, 0 to 13062\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   competencia       13063 non-null  object\n",
            " 1   tipo_calculo      13063 non-null  object\n",
            " 2   departamento      13012 non-null  object\n",
            " 3   vinculo           13063 non-null  object\n",
            " 4   nome_funcionario  13063 non-null  object\n",
            " 5   cpf               13063 non-null  object\n",
            " 6   codigo_rubrica    13063 non-null  object\n",
            " 7   nome_rubrica      13063 non-null  object\n",
            " 8   tipo_rubrica      13063 non-null  object\n",
            " 9   valor_rubrica     13063 non-null  object\n",
            "dtypes: object(10)\n",
            "memory usage: 1020.7+ KB\n",
            "Iniciando tratamento de tipos para BASE_FOPAG_DETALHADA_RUBRICAS.csv...\n",
            "Tratando tipo de data: competencia\n",
            "  -> Formato com '-' detectado. Usando parser nativo (YYYY-MM-DD).\n",
            "Tratando tipo: valor_rubrica (String -> Decimal)\n",
            "Tratando tipo: cpf (String -> String Limpa)\n",
            "Limpando coluna de texto: tipo_calculo\n",
            "Limpando coluna de texto: departamento\n",
            "Limpando coluna de texto: vinculo\n",
            "Limpando coluna de texto: nome_funcionario\n",
            "Limpando coluna de texto: codigo_rubrica\n",
            "Limpando coluna de texto: nome_rubrica\n",
            "Limpando coluna de texto: tipo_rubrica\n",
            "Tratamento de tipos finalizado.\n",
            "--- Info de BASE_FOPAG_DETALHADA_RUBRICAS.csv (APÓS tratamento):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13063 entries, 0 to 13062\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   competencia       13063 non-null  object\n",
            " 1   tipo_calculo      13063 non-null  object\n",
            " 2   departamento      13012 non-null  object\n",
            " 3   vinculo           13063 non-null  object\n",
            " 4   nome_funcionario  13063 non-null  object\n",
            " 5   cpf               13063 non-null  object\n",
            " 6   codigo_rubrica    13063 non-null  object\n",
            " 7   nome_rubrica      13063 non-null  object\n",
            " 8   tipo_rubrica      13063 non-null  object\n",
            " 9   valor_rubrica     13063 non-null  object\n",
            "dtypes: object(10)\n",
            "memory usage: 1020.7+ KB\n",
            "Competências a serem carregadas: [datetime.date(2023, 1, 1) datetime.date(2023, 4, 1)\n",
            " datetime.date(2024, 4, 1) datetime.date(2023, 5, 1)\n",
            " datetime.date(2024, 5, 1) datetime.date(2023, 6, 1)\n",
            " datetime.date(2024, 6, 1) datetime.date(2024, 8, 1)\n",
            " datetime.date(2024, 9, 1) datetime.date(2023, 12, 1)\n",
            " datetime.date(2025, 6, 1) datetime.date(2023, 3, 1)\n",
            " datetime.date(2025, 9, 1) datetime.date(2025, 10, 1)\n",
            " datetime.date(2024, 1, 1) datetime.date(2025, 4, 1)\n",
            " datetime.date(2023, 7, 1) datetime.date(2025, 7, 1)\n",
            " datetime.date(2023, 8, 1) datetime.date(2023, 9, 1)\n",
            " datetime.date(2023, 2, 1) datetime.date(2025, 1, 1)\n",
            " datetime.date(2024, 2, 1) datetime.date(2025, 2, 1)\n",
            " datetime.date(2024, 3, 1) datetime.date(2024, 7, 1)\n",
            " datetime.date(2025, 8, 1) datetime.date(2024, 10, 1)\n",
            " datetime.date(2023, 10, 1) datetime.date(2023, 11, 1)\n",
            " datetime.date(2024, 11, 1) datetime.date(2024, 12, 1)\n",
            " datetime.date(2025, 5, 1)]\n",
            "Tabela '\"FOPAG_Long\".\"fopag_rubricas_detalhe\"' não existe. Criando e inserindo (primeira carga)...\n",
            "SUCESSO! Tabela '\"FOPAG_Long\".\"fopag_rubricas_detalhe\"' criada com tipos corretos e dados carregados.\n",
            "\n",
            "Processo de carga incremental finalizado.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text, inspect\n",
        "# Importa os tipos de dados do SQLAlchemy\n",
        "from sqlalchemy.types import String, Date, Numeric\n",
        "from decimal import Decimal, InvalidOperation\n",
        "import io\n",
        "import re\n",
        "\n",
        "print(\"Iniciando processo de carga INCREMENTAL da FOPAG para o Postgres...\")\n",
        "\n",
        "# --- PASSO 1: CONFIGURAÇÃO DO BANCO ---\n",
        "DB_USER = 'postgres'\n",
        "DB_PASS = 'joao1502'\n",
        "DB_HOST = 'localhost'\n",
        "DB_PORT = '5432'\n",
        "DB_NAME = 'postgres'\n",
        "DB_SCHEMA = 'FOPAG_Long' # Schema que você quer usar\n",
        "\n",
        "connection_string = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        "\n",
        "try:\n",
        "    engine = create_engine(connection_string)\n",
        "    inspector = inspect(engine)\n",
        "    print(f\"Conexão com o banco '{DB_NAME}' estabelecida com sucesso.\")\n",
        "\n",
        "    # Garante que o schema exista\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS \\\"{DB_SCHEMA}\\\"\"))\n",
        "        print(f\"Schema '{DB_SCHEMA}' verificado/criado com sucesso.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao conectar ou criar schema: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- PASSO 2: DEFINIÇÃO EXPLÍCITA DOS SCHEMAS DAS TABELAS ---\n",
        "# Define os tipos de coluna corretos para o SQL\n",
        "\n",
        "schema_totais = {\n",
        "    'competencia': Date(),\n",
        "    'tipo_calculo': String(),\n",
        "    'departamento': String(),\n",
        "    'vinculo': String(),\n",
        "    'nome_funcionario': String(),\n",
        "    'situacao': String(),\n",
        "    'data_demissao': Date(),\n",
        "    'motivo_demissao': String(),\n",
        "    'cargo': String(),\n",
        "    'data_admissao': Date(),\n",
        "    'cpf': String(11),\n",
        "    'salario_contratual': Numeric(10, 2),\n",
        "    'total_proventos': Numeric(10, 2),\n",
        "    'total_descontos': Numeric(10, 2),\n",
        "    'valor_liquido': Numeric(10, 2),\n",
        "    'base_inss': Numeric(10, 2),\n",
        "    'base_fgts': Numeric(10, 2),\n",
        "    'valor_fgts': Numeric(10, 2),\n",
        "    'base_irrf': Numeric(10, 2)\n",
        "}\n",
        "\n",
        "schema_rubricas = {\n",
        "    'competencia': Date(),\n",
        "    'tipo_calculo': String(),\n",
        "    'departamento': String(),\n",
        "    'vinculo': String(),\n",
        "    'nome_funcionario': String(),\n",
        "    'cpf': String(11),\n",
        "    'codigo_rubrica': String(),\n",
        "    'nome_rubrica': String(),\n",
        "    'tipo_rubrica': String(),\n",
        "    'valor_rubrica': Numeric(10, 2)\n",
        "}\n",
        "\n",
        "# --- PASSO 3: FUNÇÕES DE TRATAMENTO DE TIPOS (VERSÃO CORRIGIDA) ---\n",
        "\n",
        "def clean_text(series):\n",
        "    \"\"\"Limpa uma série de texto (object) de forma segura.\"\"\"\n",
        "    if series.dtype == 'object':\n",
        "        series = series.str.strip()\n",
        "        series = series.str.replace(u'\\xa0', '', regex=False)\n",
        "        # Substitui \"N/A\" e strings vazias por None (que vira NULL)\n",
        "        series = series.replace(['N/A', '', 'nan', 'None'], None)\n",
        "    return series\n",
        "\n",
        "def para_decimal(valor_str):\n",
        "    \"\"\"Converte uma string (já limpa) para Decimal.\"\"\"\n",
        "    if valor_str is None or pd.isna(valor_str):\n",
        "        return None\n",
        "    try:\n",
        "        # A limpeza de \\xa0 e strip já foi feita\n",
        "        # Apenas removemos pontos e trocamos vírgula\n",
        "        return Decimal(valor_str.replace('.', '').replace(',', '.'))\n",
        "    except (InvalidOperation, ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def tratar_tipos_dataframe_csv(df, nome_arquivo):\n",
        "    \"\"\"\n",
        "    Função de tratamento final, ponto-a-ponto, baseada na análise dos arquivos.\n",
        "    \"\"\"\n",
        "    print(f\"Iniciando tratamento de tipos para {nome_arquivo}...\")\n",
        "\n",
        "    # --- [CORREÇÃO] COLUNAS DE DATA (TODAS AS DATAS) ---\n",
        "    # Lista de todas as colunas de data que seus arquivos podem ter\n",
        "    colunas_data = ['competencia', 'data_admissao', 'data_demissao']\n",
        "\n",
        "    for col in colunas_data:\n",
        "        if col in df.columns:\n",
        "            # 1. Limpa a coluna de texto\n",
        "            print(f\"Tratando tipo de data: {col}\")\n",
        "            df[col] = clean_text(df[col])\n",
        "\n",
        "            # 2. Pega o primeiro valor para checar o formato\n",
        "            first_valid = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n",
        "\n",
        "            # 3. Aplica a regra de conversão correta\n",
        "            if first_valid and '/' in first_valid:\n",
        "                # Lógica para formato com barra (ex: \"01/2023\" ou \"02/01/2023\")\n",
        "                print(f\"  -> Formato com '/' detectado. Usando regra DD/MM/YYYY ou MM/YYYY.\")\n",
        "                if col == 'competencia':\n",
        "                    # Formato MM/YYYY\n",
        "                    df[col] = pd.to_datetime('01/' + df[col], format='%d/%m/%Y', errors='coerce')\n",
        "                else:\n",
        "                    # Formato DD/MM/YYYY\n",
        "                    df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "            elif first_valid and '-' in first_valid:\n",
        "                # Lógica para formato com traço (ex: \"2023-01-01\")\n",
        "                print(f\"  -> Formato com '-' detectado. Usando parser nativo (YYYY-MM-DD).\")\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce') # Parser nativo\n",
        "\n",
        "            else:\n",
        "                 print(f\"  -> Nenhum formato detectado ou coluna vazia. Convertendo com parser nativo.\")\n",
        "                 df[col] = pd.to_datetime(df[col], errors='coerce') # Parser nativo\n",
        "\n",
        "            # 4. Converte para AAAA-MM-DD para o banco\n",
        "            df[col] = df[col].dt.date\n",
        "\n",
        "    # --- COLUNAS MONETÁRIAS ---\n",
        "    colunas_monetarias = [\n",
        "        'salario_contratual', 'total_proventos', 'total_descontos',\n",
        "        'valor_liquido', 'base_inss', 'base_fgts', 'valor_fgts',\n",
        "        'base_irrf', 'valor_rubrica'\n",
        "    ]\n",
        "    for col in colunas_monetarias:\n",
        "        if col in df.columns:\n",
        "            print(f\"Tratando tipo: {col} (String -> Decimal)\")\n",
        "            df[col] = clean_text(df[col]) # Limpa primeiro\n",
        "            df[col] = df[col].apply(para_decimal) # Converte depois\n",
        "\n",
        "    # --- COLUNA CPF ---\n",
        "    if 'cpf' in df.columns:\n",
        "        print(\"Tratando tipo: cpf (String -> String Limpa)\")\n",
        "        df['cpf'] = clean_text(df['cpf'])\n",
        "        df['cpf'] = df['cpf'].str.replace(r'[^\\d]', '', regex=True)\n",
        "\n",
        "    # --- COLUNAS DE TEXTO RESTANTES ---\n",
        "    colunas_texto = [\n",
        "        'tipo_calculo', 'departamento', 'vinculo', 'nome_funcionario',\n",
        "        'situacao', 'motivo_demissao', 'cargo', 'codigo_rubrica',\n",
        "        'nome_rubrica', 'tipo_rubrica'\n",
        "    ]\n",
        "    for col in colunas_texto:\n",
        "        if col in df.columns:\n",
        "            print(f\"Limpando coluna de texto: {col}\")\n",
        "            df[col] = clean_text(df[col])\n",
        "\n",
        "    print(\"Tratamento de tipos finalizado.\")\n",
        "    return df\n",
        "\n",
        "# --- PASSO 4: DEFINIÇÃO DOS ARQUIVOS ---\n",
        "arquivos_para_carregar = {\n",
        "    'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv': 'fopag_totais',\n",
        "    'BASE_FOPAG_DETALHADA_RUBRICAS.csv': 'fopag_rubricas_detalhe'\n",
        "}\n",
        "\n",
        "# --- PASSO 5: LOOP DE ETL COM LÓGICA DE SCHEMA E DTYPE ---\n",
        "for nome_arquivo, nome_tabela in arquivos_para_carregar.items():\n",
        "\n",
        "    print(f\"\\n--- Processando arquivo: {nome_arquivo} ---\")\n",
        "\n",
        "    try:\n",
        "        # --- (E) EXTRACT ---\n",
        "        df = pd.read_csv(\n",
        "            nome_arquivo,\n",
        "            sep=';',\n",
        "            na_values=['N/A', 'NaN', ''],\n",
        "            dtype=str  # Força tudo a ser lido como string\n",
        "        )\n",
        "\n",
        "        # --- [VALIDAÇÃO DA LEITURA - CONFORME SOLICITADO] ---\n",
        "        print(f\"Arquivo lido com sucesso (como texto). {len(df)} linhas carregadas.\")\n",
        "        print(f\"--- Amostra (head) de {nome_arquivo} (dados crus):\")\n",
        "        print(df.head())\n",
        "        print(f\"--- Info de {nome_arquivo} (dados crus):\")\n",
        "        df.info()\n",
        "        # --- FIM DA VALIDAÇÃO ---\n",
        "\n",
        "        # --- (T) TRANSFORM ---\n",
        "        df_tratado = tratar_tipos_dataframe_csv(df.copy(), nome_arquivo)\n",
        "\n",
        "        print(f\"--- Info de {nome_arquivo} (APÓS tratamento):\")\n",
        "        df_tratado.info() # Mostra os tipos de dados convertidos\n",
        "\n",
        "        dtype_map = schema_totais if nome_tabela == 'fopag_totais' else schema_rubricas\n",
        "\n",
        "        competencias_no_df = df_tratado['competencia'].dropna().unique()\n",
        "\n",
        "        if len(competencias_no_df) == 0:\n",
        "            print(f\"AVISO: Nenhuma competência válida encontrada após tratamento. Pulando carga.\")\n",
        "            print(\"Amostra da coluna 'competencia' ORIGINAL (antes da falha):\")\n",
        "            print(df['competencia'].head(10))\n",
        "            continue\n",
        "\n",
        "        print(f\"Competências a serem carregadas: {competencias_no_df}\")\n",
        "\n",
        "        # --- (L) LOAD ---\n",
        "\n",
        "        table_exists = inspector.has_table(nome_tabela, schema=DB_SCHEMA)\n",
        "\n",
        "        if table_exists:\n",
        "            print(f\"Tabela '\\\"{DB_SCHEMA}\\\".\\\"{nome_tabela}\\\"' já existe. Executando Delete-then-Append...\")\n",
        "\n",
        "            with engine.begin() as conn:\n",
        "                sql_delete = text(f\"\"\"\n",
        "                    DELETE FROM \"{DB_SCHEMA}\".\"{nome_tabela}\"\n",
        "                    WHERE competencia IN :competencias_list\n",
        "                \"\"\")\n",
        "                conn.execute(sql_delete, {\"competencias_list\": tuple(competencias_no_df)})\n",
        "                print(\"DELETE executado. Iniciando INSERT (append)...\")\n",
        "\n",
        "                df_tratado.to_sql(\n",
        "                    name=nome_tabela,\n",
        "                    con=conn,\n",
        "                    if_exists='append',\n",
        "                    index=False,\n",
        "                    schema=DB_SCHEMA,\n",
        "                    dtype=dtype_map\n",
        "                )\n",
        "\n",
        "                print(f\"SUCESSO! Dados (append) carregados na tabela '\\\"{DB_SCHEMA}\\\".\\\"{nome_tabela}\\\"'.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Tabela '\\\"{DB_SCHEMA}\\\".\\\"{nome_tabela}\\\"' não existe. Criando e inserindo (primeira carga)...\")\n",
        "\n",
        "            df_tratado.to_sql(\n",
        "                name=nome_tabela,\n",
        "                con=engine,\n",
        "                if_exists='fail',\n",
        "                index=False,\n",
        "                schema=DB_SCHEMA,\n",
        "                dtype=dtype_map # Essencial: Cria a tabela com os tipos corretos\n",
        "            )\n",
        "\n",
        "            print(f\"SUCESSO! Tabela '\\\"{DB_SCHEMA}\\\".\\\"{nome_tabela}\\\"' criada com tipos corretos e dados carregados.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERRO: Arquivo '{nome_arquivo}' não encontrado.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO GERAL ao processar o arquivo '{nome_arquivo}': {e}\")\n",
        "\n",
        "print(\"\\nProcesso de carga incremental finalizado.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}