# ğŸ§© Projeto de ETL e Data Warehouse para People Analytics (Solides API & FOPAG PDFs)

Este projeto implementa um **pipeline de dados completo** para centralizar informaÃ§Ãµes de **Recursos Humanos**, unificando dados **cadastrais (API Solides)** e **financeiros (Folha de Pagamento - PDFs)** em um **Data Warehouse dimensional (PostgreSQL)**, pronto para anÃ¡lise em **Power BI**.

---

## ğŸš€ 1. VisÃ£o Geral da Arquitetura

O processo Ã© dividido em duas etapas principais, orquestradas por **dois notebooks Python**:

- **`AutomaÃ§Ã£o_FOPAG.ipynb`** â†’ Parser de PDFs da folha de pagamento.
- **`Carga_API_Solides.ipynb`** â†’ Orquestrador ETL (API + CSV â†’ PostgreSQL).

### ğŸ§  Fluxo do Processo (Mermaid)
```mermaid
graph LR

subgraph "Fonte 1: Folha de Pagamento (PDF)"
    A["pastas/FOPAG/*.pdf"] -->|Leitura| B["Automacao_FOPAG.ipynb"]
    B -->|Gera| C["BASE_FOPAG_CONSOLIDADA_TOTAIS.csv"]
    B -->|Gera| D["BASE_FOPAG_DETALHADA_RUBRICAS.csv"]
end

subgraph "Fonte 2: Dados Cadastrais (API)"
    E["Solides API /colaboradores"] -->|Extracao| F["Carga_API_Solides.ipynb"]
end

subgraph "Processo de Carga (ETL Python)"
    C -->|Leitura| F
    D -->|Leitura| F
    F -->|Gera e Carrega| J["dim_calendario"]
    F -->|Staging e UPSERT| G["dim_colaboradores"]
    F -->|Carga Incremental| H["fato_folha_consolidada"]
    F -->|Carga Incremental| I["fato_folha_detalhada"]
end

subgraph "Data Warehouse (PostgreSQL)"
    DW["PostgreSQL DW (Schema: FOPAG)"]
    G --> DW
    H --> DW
    I --> DW
    J --> DW
end

subgraph "Consumo (Power BI)"
    DW -->|Importar| K["Power BI / Power Query"]
    K -->|Transformar e Modelar| L["Modelo de Dados Star Schema"]
    L -->|Visualizar| M["Dashboard de People Analytics"]
end
```


# âš™ï¸ 2. Componentes do Projeto
## ğŸ§¾ AutomaÃ§Ã£o_FOPAG.ipynb (Parser de PDF)

Objetivo: Extrair informaÃ§Ãµes de folhas de pagamento em PDF e gerar arquivos CSV estruturados.
Â´Â´Â´Tecnologias: pdfplumber, pandas, reÂ´Â´Â´

## Principais Funcionalidades:

Leitura em lote: Varre o diretÃ³rio FOPAG/ e processa todos os PDFs.
ExtraÃ§Ã£o robusta: Usa RegEx para identificar blocos de texto e extrair campos de cada colaborador.
Mapeamento de rubricas: Traduz cÃ³digos de rubricas em descriÃ§Ãµes padronizadas e classifica como Provento ou Desconto.
Compatibilidade com diferentes layouts: Detecta holerites e recibos de fÃ©rias automaticamente.

SaÃ­das:

`BASE_FOPAG_CONSOLIDADA_TOTAIS.csv`

`BASE_FOPAG_DETALHADA_RUBRICAS.csv`

## ğŸ§ (ETL do Data Warehouse) - `Carga_API_Solides.ipynb`

### Objetivo: Centralizar e estruturar os dados no PostgreSQL.

### Tecnologias: `requests`, `pandas`, `sqlalchemy`, `python-dotenv`

### Etapas ETL:

### ğŸ”¹ Extract

API Solides: Extrai colaboradores do endpoint `/colaboradores` e detalhes de `/colaboradores/{id}.`

CSVs: LÃª as bases geradas pelo parser de PDF.

### ğŸ”¹ Transform

NormalizaÃ§Ã£o e renomeaÃ§Ã£o de colunas.

ConversÃ£o de valores monetÃ¡rios e tratamento de CPFs.

GeraÃ§Ã£o da tabela de calendÃ¡rio (dim_calendario).

### ğŸ”¹ Load

`dim_colaboradores (UPSERT)`: Atualiza ou insere colaboradores.

`fato_folha_* (Incremental)`: EstratÃ©gia DELETE-then-INSERT por competÃªncia.

CriaÃ§Ã£o automÃ¡tica de colunas: Usa ALTER TABLE ... ADD COLUMN IF NOT EXISTS para evitar erros.

# ğŸ§± 3. Estrutura do Data Warehouse

O modelo segue o padrÃ£o Star Schema (Esquema Estrela).

### ğŸ§© **DimensÃµes**
|Tabela	|Fonte	|DescriÃ§Ã£o|
|----------|---------|--------|
|`dim_colaboradores`|	API Solides	Dados cadastrais e demogrÃ¡ficos |(nome, cargo, departamento, salÃ¡rio, etc.)|
|`dim_calendario`|	SQL	Datas com atributos de tempo |(ano, mÃªs, trimestre, etc.)|
### ğŸ“Š **Fatos**
|Tabela	Fonte |Granularidade	|DescriÃ§Ã£o|
|-----------|-------------|---------------|
`fato_folha_consolidada`|	Totais CSV	|1 linha por colaborador/mÃªs	|Proventos, Descontos, Bases INSS/IRRF|
`fato_folha_detalhada`	|Rubricas CSV	|1 linha por rubrica/mÃªs/colaborador|	Rubricas individuais (ex: Horas Extras, INSS)|

#âš¡ 4. Como Executar o Projeto
###ğŸ”§ PrÃ©-requisitos

- Python 3.10+
- PostgreSQL
- Token de acesso Ã  API Solides

### ğŸ§° ConfiguraÃ§Ã£o
### 1ï¸âƒ£ Clonar o RepositÃ³rio
`git clone <url-do-repositorio>
cd <nome-do-repositorio>`

### 2ï¸âƒ£ Criar Ambiente Virtual
`python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate      # Windows`

### 3ï¸âƒ£ Instalar DependÃªncias

```requirements.txt
pandas
pdfplumber
sqlalchemy
psycopg2-binary
requests
python-dotenv
pip install -r requirements.txt
```

### 4ï¸âƒ£ Criar o arquivo .env
# Credenciais da API Solides
`SOLIDES_API_TOKEN="seu_token_aqui"`

# Banco de Dados PostgreSQL
```DB_USER="seu_usuario"
DB_PASS="sua_senha"
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="postgres"
DB_SCHEMA="FOPAG"
```
### ğŸš€ ExecuÃ§Ã£o

1-Coloque os PDFs na pasta FOPAG/

2-Execute o notebook AutomaÃ§Ã£o_FOPAG.ipynb

3-Execute o notebook Carga_API_Solides.ipynb

4-Conecte o Power BI ao schema FOPAG do PostgreSQL

### ğŸ§® 5. Boas PrÃ¡ticas e Destaques TÃ©cnicos

âœ… SeguranÃ§a: Credenciais isoladas em .env

âœ… IdempotÃªncia: UPSERT em dim_colaboradores

âœ… Carga Incremental: DELETE-then-INSERT por competÃªncia

âœ… TolerÃ¢ncia a mudanÃ§as no schema: ADD COLUMN IF NOT EXISTS

âœ… Modelagem em Estrela: ideal para anÃ¡lise no Power BI

### ğŸ” 6. PrÃ³ximos Passos (Melhorias Futuras)

- Unificar scripts: PDF â†’ DataFrame â†’ Postgres (sem CSV intermediÃ¡rio).

- Adicionar logging estruturado: usando logging para auditoria e controle.

- SubstituiÃ§Ã£o de chaves no ETL: inserir colaborador_sk diretamente nas Fatos no PostgreSQL.

### ğŸ“Š 7. Modelagem e TransformaÃ§Ãµes no Power BI

O modelo no Power BI implementa um Star Schema otimizado, com substituiÃ§Ã£o de chaves e normalizaÃ§Ã£o.

### ğŸ” TransformaÃ§Ãµes no Power Query
ğŸ”¸ SubstituiÃ§Ã£o de Chaves (Key Substitution)

Merge entre `fato_* e dim_colaboradores usando cpf_csv = cpf.`

ExtraÃ§Ã£o da chave `colaborador_sk para substituir cpf_csv.`

ğŸ”¸ CriaÃ§Ã£o de DimensÃµes (Snowflaking)

`dim_Cargo`, `dim_Departamento`, `dim_Nivel_Educacional criadas referenciando dim_colaboradores`.

### ğŸ”— Relacionamentos (Modelo Estrela)
``` mermaid
erDiagram
    dim_calendario {
        date data
    }

    fato_folha_consolidada {
        int competencia
        int colaborador_sk
    }

    fato_folha_detalhada {
        int competencia
        int colaborador_sk
    }

    dim_colaboradores {
        int colaborador_sk
        int id_Departamento
        int id_Cargo
        int id_Nivel_Educacional
    }

    dim_Departamento {
        int id_Departamento
    }

    dim_Cargo {
        int id_Cargo
    }

    dim_Nivel_Educacional {
        int id_Nivel_Educacional
    }

    %% Relacionamentos 1:N
    dim_calendario ||--o{ fato_folha_consolidada : "data = competencia"
    dim_calendario ||--o{ fato_folha_detalhada : "data = competencia"

    dim_colaboradores ||--o{ fato_folha_consolidada : "colaborador_sk"
    dim_colaboradores ||--o{ fato_folha_detalhada : "colaborador_sk"

    dim_Departamento ||--o{ dim_colaboradores : "id_Departamento"
    dim_Cargo ||--o{ dim_colaboradores : "id_Cargo"
    dim_Nivel_Educacional ||--o{ dim_colaboradores : "id_Nivel_Educacional"
```
ğŸ§  Autor

JoÃ£o Pedro dos Santos Santana
ğŸ“Š Analista de BI JÃºnior & Entusiasta de People Analytics
ğŸ“§ LinkedIn
 | Notion

Projeto desenvolvido para automatizar o fluxo de dados de RH, reduzir retrabalho manual e fortalecer anÃ¡lises de People Analytics com dados confiÃ¡veis e estruturados.
