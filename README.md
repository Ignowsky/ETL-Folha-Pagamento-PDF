# üöÄ ETL de Folha de Pagamento (PDF para PostgreSQL)

> Um pipeline de dados em Python completo para automatizar a extra√ß√£o, transforma√ß√£o e carga (ETL) de relat√≥rios complexos de folha de pagamento em PDF para um banco de dados PostgreSQL.

Este projeto foi desenhado para processar diret√≥rios contendo m√∫ltiplos arquivos PDF de folha de pagamento (holerites, recibos de f√©rias, 13¬∫ sal√°rio), extrair dados de cada funcion√°rio e de cada rubrica, e carregar tudo de forma estruturada em um data warehouse no PostgreSQL.

```mermaid
graph LR;
    subgraph Fase 1: Parsing e Estrutura√ß√£o;
        A[üìÇ Diret√≥rio de PDFs] -- 1. Leitura --> B(Script de Parsing Python);
        B -- 2. Aplica Regex e L√≥gica --> C{MAPEAMENTO_ORIGINAL};
        B --> D[üìÑ BASE_FOPAG_CONSOLIDADA_TOTAIS.csv];
        B --> E[üìÑ BASE_FOPAG_DETALHA_RUBRICAS.csv];
    end;

subgraph Fase 2: Carga no Data Warehouse;
        D -- 3. Leitura e Limpeza --> F(Script de Carga Python);
        E -- 3. Leitura e Limpeza --> F;
        F -- 4. Conex√£o Segura (SQLAlchemy) --> G[üêò Banco de Dados PostgreSQL];
        G -- 5. Delete-then-Append --> H[Tabela - FOPAG.fopag_totais];
        G -- 5. Delete-then-Append --> I[Tabela - FOPAG.fopag_rubricas_detalhe];
    end;
```

## ‚ú® Funcionalidades Principais

* **Extra√ß√£o de PDF:** Utiliza o `pdfplumber` para ler e extrair texto de arquivos PDF.
* **Parsing Robusto com Regex:** Emprega express√µes regulares (`re`) para lidar com layouts de PDF complexos e vari√°veis, identificando corretamente cada funcion√°rio e seus dados, mesmo em documentos com m√∫ltiplos holerites.
* **Mapeamento de Rubricas:** Inclui um dicion√°rio de mapeamento central (`MAPEAMENTO_ORIGINAL`) que traduz c√≥digos internos de pagamento (ex: '101', '998') para descri√ß√µes padronizadas (ex: 'P_Salario_Base', 'D_INSS').
* **Gera√ß√£o de Duas Bases:** O script de parsing gera duas sa√≠das principais:
    1.  **Consolidada:** Uma linha por funcion√°rio por compet√™ncia, com todos os totais (Proventos, Descontos, L√≠quido, Bases de C√°lculo).
    2.  **Detalhada:** Uma linha para cada rubrica (item) de cada funcion√°rio, permitindo an√°lises granulares.
* **ETL Incremental para Postgres:** O segundo script (`c√©lula 4`) implementa um pipeline de ETL que carrega os CSVs gerados para o PostgreSQL.
* **Carga Idempotente (Delete-then-Append):** A carga no banco √© segura para ser re-executada. O script deleta os dados de uma compet√™ncia que j√° existe antes de inserir os novos, evitando duplicatas e garantindo que os dados estejam sempre atualizados.
* **Tipagem de Dados SQL:** Define explicitamente os tipos de dados no PostgreSQL (`Date`, `Numeric(10, 2)`) para garantir a integridade e precis√£o dos dados financeiros.

## ‚öôÔ∏è Como Funciona: O Pipeline de Duas Etapas

O processo √© dividido em duas grandes etapas, ambas contidas no notebook `leitor_fopag.ipynb`.

### Etapa 1: Parsing de PDF para CSV

A primeira parte do pipeline (c√©lula principal) foca em ler os PDFs brutos e transform√°-los em arquivos CSV estruturados.

1.  **Leitura:** O script varre o diret√≥rio `FOPAG/` em busca de todos os arquivos `.pdf`.
2.  **Extra√ß√£o:** O texto de cada PDF √© extra√≠do.
3.  **Divis√£o:** O texto √© dividido em blocos, um para cada funcion√°rio, usando regex para identificar os cabe√ßalhos.
4.  **Extra√ß√£o de Dados:** Para cada bloco de funcion√°rio, o script usa regex para extrair:
    * Dados do Cabe√ßalho (Nome, CPF, Cargo, Admiss√£o, Departamento).
    * Dados do Rodap√© (Total Proventos, Total Descontos, L√≠quido, Bases de INSS, FGTS, IRRF). A l√≥gica √© robusta para encontrar esses valores tanto em holerites mensais quanto em recibos de f√©rias.
    * Dados da Tabela de Rubricas (C√≥digo, Descri√ß√£o, Valor).
5.  **Mapeamento:** As rubricas extra√≠das s√£o traduzidas usando o `MAPEAMENTO_CODIGOS`.
6.  **Sa√≠da:** Os dados s√£o salvos em dois arquivos:
    * `BASE_FOPAG_CONSOLIDADA_TOTAIS.csv`
    * `BASE_FOPAG_DETALHADA_RUBRICAS.csv`

### Etapa 2: Carga dos CSVs para o PostgreSQL

A segunda parte do pipeline (c√©lula 4) pega os CSVs gerados e os carrega no banco de dados.

1.  **Extra√ß√£o (dos CSVs):** Os dois arquivos CSV s√£o lidos com o `pandas`, for√ßando todos os campos como `string` para um tratamento de tipos controlado.
2.  **Transforma√ß√£o:** Uma fun√ß√£o `tratar_tipos_dataframe_csv` realiza a limpeza final:
    * Converte colunas de data (ex: `01/10/2025` ou `2025-10-01`) para o formato `Date` do SQL.
    * Converte colunas monet√°rias (ex: `1.234,56`) para `Decimal`, garantindo precis√£o.
    * Limpa e padroniza campos de texto e o CPF.
3.  **Carga (Load):**
    * O script se conecta ao banco PostgreSQL usando credenciais de um arquivo `.env`.
    * Ele verifica as compet√™ncias (ex: '2025-10-01') presentes nos arquivos CSV.
    * **L√≥gica Incremental:** Ele executa um `DELETE FROM tabela WHERE competencia IN (...)` para remover quaisquer dados dessas compet√™ncias que j√° existam no banco.
    * **Append:** Por fim, ele usa `to_sql(if_exists='append')` para inserir os novos dados tratados nas tabelas `FOPAG.fopag_totais` e `FOPAG.fopag_rubricas_detalhe`.

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3.11+**
* **Pandas:** Para manipula√ß√£o e estrutura√ß√£o dos dados.
* **PDFPlumber:** Para extra√ß√£o de texto de PDFs.
* **SQLAlchemy:** Para ORM e defini√ß√£o de tipos de dados na conex√£o com o banco.
* **Psycopg2-binary:** Driver de conex√£o com o PostgreSQL.
* **Python-Dotenv:** Para gerenciamento de credenciais de banco de dados.
* **PostgreSQL:** O banco de dados de destino.

## üèÅ Como Executar o Projeto

### 1. Pr√©-requisitos

* Python 3.11 ou superior
* Um servidor PostgreSQL acess√≠vel

### 2. Configura√ß√£o do Ambiente

1.  Clone este reposit√≥rio:
    ```bash
    git clone [https://github.com/seu-usuario/ETL-Folha-Pagamento-PDF.git](https://github.com/seu-usuario/ETL-Folha-Pagamento-PDF.git)
    cd ETL-Folha-Pagamento-PDF
    ```

2.  Crie e ative um ambiente virtual:
    ```bash
    python -m venv .venv
    # Windows
    .\.venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate
    ```

3.  Crie um arquivo `requirements.txt` com o seguinte conte√∫do:
    ```
    pandas
    pdfplumber
    SQLAlchemy
    psycopg2-binary
    python-dotenv
    ```

4.  Instale as depend√™ncias:
    ```bash
    pip install -r requirements.txt
    ```

5.  Crie um arquivo `.env` na raiz do projeto para suas credenciais do PostgreSQL. O script espera por estas vari√°veis:
    ```ini
    # .env
    DB_USER="seu_usuario_postgres"
    DB_PASS="sua_senha_segura"
    DB_HOST="localhost"
    DB_PORT="5432"
    DB_NAME="postgres"
    DB_SCHEMA="FOPAG"
    ```

### 3. Execu√ß√£o

1.  Crie uma pasta chamada `FOPAG` na raiz do projeto.
2.  Coloque todos os seus arquivos PDF de folha de pagamento dentro da pasta `FOPAG`.
3.  Abra o notebook `leitor_fopag.ipynb` em seu editor (VS Code, Jupyter Lab).
4.  Execute a **primeira c√©lula de instala√ß√£o** (`!pip install...`) ou pule-a, j√° que usamos o `requirements.txt`.
5.  Execute a **c√©lula principal (Etapa 1)**. Isso ir√° processar os PDFs e criar os dois arquivos CSV. Monitore o output para ver o progresso.
6.  (Opcional) Execute a **c√©lula de verifica√ß√£o (Etapa 1.5)** para checar os dados de um funcion√°rio espec√≠fico no CSV gerado.
7.  Execute a **c√©lula de carga (Etapa 2)**. Isso ir√° conectar ao seu banco de dados, criar o schema `FOPAG` (se n√£o existir) e carregar os dados nas tabelas `fopag_totais` e `fopag_rubricas_detalhe`.

Ap√≥s a execu√ß√£o, seus dados estar√£o prontos para serem consultados no PostgreSQL.
